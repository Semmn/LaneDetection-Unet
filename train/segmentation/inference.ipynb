{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/work')\n",
    "from train.utils.create_model import create_unet\n",
    "import yaml\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'loss': {'loss_type': 'lane_dice',\n",
       "   'lane_dice': {'num_cls': 5},\n",
       "   'cross_entropy': {'weights': 'None',\n",
       "    'ignore_index': -100,\n",
       "    'reduction': 'mean',\n",
       "    'label_smoothing': 0.0},\n",
       "   'dice': {'weights': 'None', 'num_cls': 5}},\n",
       "  'optimizer': {'optimizer_type': 'adamw',\n",
       "   'adamw': {'lr': 0.0006,\n",
       "    'betas': [0.9, 0.999],\n",
       "    'eps': '1e-8',\n",
       "    'weight_decay': 0.01,\n",
       "    'amsgrad': False,\n",
       "    'maximize': False,\n",
       "    'foreach': 'None',\n",
       "    'capturable': False,\n",
       "    'differentialable': False,\n",
       "    'fused': 'None'},\n",
       "   'adam': {'lr': 0.0006,\n",
       "    'betas': [0.9, 0.999],\n",
       "    'eps': '1e-8',\n",
       "    'weight_decay': 0.01,\n",
       "    'amsgrad': False,\n",
       "    'foreach': 'None',\n",
       "    'maximize': False,\n",
       "    'capturaable': False,\n",
       "    'differentialable': False,\n",
       "    'fused': 'None'},\n",
       "   'rmsprop': {'lr': 0.0006,\n",
       "    'alpha': 0.99,\n",
       "    'eps': '1e-8',\n",
       "    'weight_decay': 0,\n",
       "    'momentum': 0,\n",
       "    'centered': False,\n",
       "    'capturable': False,\n",
       "    'foreach': 'None',\n",
       "    'maximize': False,\n",
       "    'differentialable': False},\n",
       "   'sgd': {'lr': 0.0006,\n",
       "    'momentum': 0,\n",
       "    'dampening': 0,\n",
       "    'weight_decay': 0,\n",
       "    'nesterov': False,\n",
       "    'maximize': False,\n",
       "    'foreach': 'None',\n",
       "    'differentiable': False,\n",
       "    'fused': 'None'}},\n",
       "  'scheduler': {'scheduler_type': 'cosine_warmup_restart',\n",
       "   'cosine_warmup_restart': {'epoch': 60,\n",
       "    'lr_min': 1e-05,\n",
       "    'decay': 0.2,\n",
       "    'cycle_mul': 1.0,\n",
       "    'cycle_limit': 100,\n",
       "    'warmup_lr': 10,\n",
       "    'warmup_lr_init': 1e-06,\n",
       "    'step_size': 30},\n",
       "   'one_cycle': {'epoch': 60,\n",
       "    'max_lr': 0.0006,\n",
       "    'pct': 0.3,\n",
       "    'anneal_strategy': 'cos',\n",
       "    'div': 60,\n",
       "    'f_div': 10000,\n",
       "    'three_phase': False}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/work/train/segmentation/train_config/segmentation_culane.yaml', 'r') as f:\n",
    "    train_config = yaml.load(f, Loader=yaml.Loader)\n",
    "train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf value:  False\n",
      "leaf value:  10000\n",
      "leaf value:  60\n",
      "leaf value:  cos\n",
      "leaf value:  0.3\n",
      "leaf value:  0.0006\n",
      "leaf value:  60\n",
      "leaf value:  cosine_warmup_restart\n",
      "leaf value:  30\n",
      "leaf value:  1e-06\n",
      "leaf value:  10\n",
      "leaf value:  100\n",
      "leaf value:  1.0\n",
      "leaf value:  0.2\n",
      "leaf value:  1e-05\n",
      "leaf value:  60\n",
      "leaf value:  cosine_warmup_restart\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  0\n",
      "leaf value:  0\n",
      "leaf value:  0\n",
      "leaf value:  0.0006\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  0\n",
      "leaf value:  0\n",
      "leaf value:  1e-08\n",
      "leaf value:  0.99\n",
      "leaf value:  0.0006\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  0.01\n",
      "leaf value:  1e-08\n",
      "leaf value:  [0.9, 0.999]\n",
      "leaf value:  0.0006\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  None\n",
      "leaf value:  False\n",
      "leaf value:  False\n",
      "leaf value:  0.01\n",
      "leaf value:  1e-08\n",
      "leaf value:  [0.9, 0.999]\n",
      "leaf value:  0.0006\n",
      "leaf value:  adamw\n",
      "leaf value:  5\n",
      "leaf value:  None\n",
      "leaf value:  0.0\n",
      "leaf value:  mean\n",
      "leaf value:  -100\n",
      "leaf value:  None\n",
      "leaf value:  5\n",
      "leaf value:  lane_dice\n",
      "Elasped time:  0.0015647411346435547\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# # dictionary takes approach to its values as depth (level) of the tree\n",
    "# # if you want to traverse all the values in the dictionary, you need to use recursive function\n",
    "# def traverse_dict(input_dict):\n",
    "    \n",
    "#     start_t = time.time()\n",
    "    \n",
    "#     # we will use FIFO style search using queue\n",
    "#     queue_k = []\n",
    "#     queue_v = []\n",
    "#     tree_keys = input_dict.keys()\n",
    "#     search_flag = True # at first, we need to search what keys they have (search root keys)\n",
    "#     sub_dict = input_dict # sub_dict is intialized as input_dict\n",
    "    \n",
    "#     level_indicator = [] # used to store the level information of the tree (pivot value of current position)\n",
    "#     key_history = [] # store popped key (not leaf node key)\n",
    "    \n",
    "#     while True:\n",
    "#         if len(level_indicator)==0: # if level indicator is empty, we are at the root level\n",
    "#             counter = 0\n",
    "#         else:\n",
    "#             counter = level_indicator[-1] # cumulative sum\n",
    "            \n",
    "#         if search_flag: # if pass then, continue to next iteration\n",
    "#             for tree_key in tree_keys:\n",
    "#                 counter += 1\n",
    "#                 queue_k.insert(0, tree_key)\n",
    "#                 queue_v.insert(0, sub_dict[tree_key])\n",
    "#             level_indicator.append(counter)\n",
    "\n",
    "#         if not isinstance(queue_v[0], dict):\n",
    "#             key = queue_k.pop(0)\n",
    "#             leaf = queue_v.pop(0)\n",
    "            \n",
    "#             if leaf == 'None': # 1. if leaf element is string 'None' we convert it to python None\n",
    "#                 leaf = None\n",
    "#             elif isinstance(leaf, str): # 2. if leaf element is string, we try to convert to float format if possible (convert '1e-8' to float 1e-8)\n",
    "#                 try:\n",
    "#                     leaf = float(leaf)\n",
    "#                 except:\n",
    "#                     pass\n",
    "            \n",
    "#             current_level = level_indicator.pop()\n",
    "#             if len(level_indicator) > 1: # in this case, leaf belongs to the sub-dictionary\n",
    "#                 if (current_level != level_indicator[-1]):\n",
    "#                     current_level -= 1 # decrease the pivot by 1\n",
    "#                     level_indicator.append(current_level)\n",
    "#                     sub_dict[key] = leaf # update the leaf value to filtered value\n",
    "#                 else: # in this case, leaf does not belong to the sub-dictionary, we need to move to the parent node (directly corresponding value is not dictionary)\n",
    "#                     access = input_dict\n",
    "#                     for k in key_history:\n",
    "#                         print(\"key: \", k)\n",
    "#                         access = access[k]\n",
    "#                     access[key] = leaf # follow the key history and save the leaf value\n",
    "#                     level_indicator.append(current_level)\n",
    "                    \n",
    "#                 current_level = level_indicator.pop()\n",
    "#                 if (current_level == level_indicator[-1]):\n",
    "#                     key_history.pop() # remove the registered key and level_indicator\n",
    "#                 else:\n",
    "#                     level_indicator.append(current_level)\n",
    "                    \n",
    "#             elif len(level_indicator) == 1: # in this case, leaf directly belongs to the root\n",
    "#                 current_level -= 1\n",
    "#                 level_indicator.append(current_level)\n",
    "#                 sub_dict[key] = leaf\n",
    "                \n",
    "#             print(\"leaf value: \", leaf) # no more depth search, left node\n",
    "#             search_flag=False\n",
    "#         else:\n",
    "#             key = queue_k.pop(0) # in case of dictionary, we need to pop the key on the stored queue\n",
    "#             sub_dict = queue_v.pop(0)\n",
    "#             tree_keys = sub_dict.keys()\n",
    "#             search_flag=True # more depth search is needed\n",
    "                        \n",
    "#             key_history.append(key) # since this is not the leaf node key, we need to store it to save the information\n",
    "#         if len(queue_v)==0 and not search_flag: # consuming all the element in queue means we search all the trees\n",
    "#             break\n",
    "    \n",
    "#     end_t = time.time()\n",
    "#     print(\"Elasped time: \", end_t-start_t)\n",
    "            \n",
    "# traverse_dict(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "UNet                                                    [1, 5, 224, 672]          --\n",
       "├─Encoder: 1-1                                          [1, 384, 7, 21]           --\n",
       "│    └─Stem: 2-1                                        [1, 48, 56, 168]          --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 48, 56, 168]          2,352\n",
       "│    └─ModuleList: 2-2                                  --                        --\n",
       "│    │    └─Stage: 3-2                                  [1, 48, 56, 168]          2,780,793\n",
       "│    │    └─Stage: 3-3                                  [1, 96, 28, 84]           1,634,994\n",
       "│    │    └─Stage: 3-4                                  [1, 192, 14, 42]          3,489,480\n",
       "│    │    └─Stage: 3-5                                  [1, 384, 7, 21]           4,653,000\n",
       "├─Decoder: 1-2                                          [1, 48, 224, 672]         --\n",
       "│    └─ModuleList: 2-3                                  --                        --\n",
       "│    │    └─DStage: 3-6                                 [1, 384, 14, 42]          18,227,376\n",
       "│    │    └─DStage: 3-7                                 [1, 192, 28, 84]          9,758,808\n",
       "│    │    └─DStage: 3-8                                 [1, 96, 56, 168]          5,605,164\n",
       "│    └─DStemNx: 2-4                                     [1, 48, 224, 672]         --\n",
       "│    │    └─ConvTranspose2d: 3-9                        [1, 48, 224, 672]         73,776\n",
       "├─Conv2d: 1-3                                           [1, 5, 224, 672]          245\n",
       "├─Softmax2d: 1-4                                        [1, 5, 224, 672]          --\n",
       "=========================================================================================================\n",
       "Total params: 46,225,988\n",
       "Trainable params: 46,225,988\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 61.92\n",
       "=========================================================================================================\n",
       "Input size (MB): 1.81\n",
       "Forward/backward pass size (MB): 474.57\n",
       "Params size (MB): 184.90\n",
       "Estimated Total Size (MB): 661.28\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/work/train/segmentation/model_config/unet.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "unet = create_unet('/work/train/segmentation/model_config/unet.yaml')\n",
    "torchinfo.summary(unet, (1, 3, 224, 672)) # torchsummary takes some gpu memory (if only loaded with cpu device, no gpu memory is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
