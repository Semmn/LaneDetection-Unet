{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('backbone')))) # to import file from under same-level directory\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('utils'))))\n",
    "\n",
    "from backbone.convnext_se.convnext_se import ConvNextV1, Stage, Stem\n",
    "from utils.conv_2d import adjust_padding_for_strided_output, DepthWiseSepConv\n",
    "from utils.stochastic_depth_drop import create_linear_p, create_uniform_p\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import torchinfo\n",
    "import albumentations\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image size:  (1640, 590)\n",
      "torch.Size([1, 3, 224, 672])\n"
     ]
    }
   ],
   "source": [
    "ex_img = Image.open(\"/work/dataset/CULane/driver_193_90frame/06051132_0638.MP4/00000.jpg\")\n",
    "print(\"original image size: \", ex_img.size)\n",
    "# detector also will have much smaller image scale than original image\n",
    "ex_img_resize = ex_img.resize([672, 224])\n",
    "ex_img = torch.tensor(np.array(ex_img_resize)).unsqueeze(0).permute(0, 3, 1, 2).float().to('cuda')\n",
    "print(ex_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnext_se for detector backbone (lane point regression + bg/fg classification for detection task)\n",
    "class DetectorBackbone(torch.nn.Module):\n",
    "    def __init__(self, num_blocks:list, input_channels:int, stem_kersz:tuple, stem_stride:tuple, \n",
    "                 img_hw:list, main_channels:list, expansion_dim:list, kernel_sz:list, stride:list, padding:list, dilation:list, groups:list, droprate:list, drop_mode:list, use_se:list, squeeze_ratio:int,\n",
    "                 transition_kersz:list, transition_stride:list, norm_mode:str, device='cuda'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks # number of blocks for each stage\n",
    "        self.input_channels = input_channels # mostly RGB 3 channel images\n",
    "        self.stem_kersz = stem_kersz # kernel size for stem layer\n",
    "        self.stem_stride = stem_stride # stride for stem layer\n",
    "        self.img_hw = img_hw # representative image height and width for each stage\n",
    "        self.main_channels = main_channels # main channels for each stage\n",
    "        self.expansion_dim = expansion_dim # expansion dimension for each stage\n",
    "        self.kernel_sz = kernel_sz # kernel size for each stage\n",
    "        self.stride=stride # stride for each stage\n",
    "        self.padding=padding # padding for each stage\n",
    "        self.dilation=dilation # dilation for each stage\n",
    "        self.groups=groups # number of groups for each stage\n",
    "        self.droprate=droprate # constant droprate for all stage\n",
    "        self.drop_mode=drop_mode # drop_mode is same for all stage\n",
    "        self.use_se=use_se # flag for using se operation in each stage\n",
    "        self.squeeze_ratio=squeeze_ratio # squeeze_ratio is same for all stage\n",
    "        self.transition_kersz=transition_kersz # transition kernel size for each stage\n",
    "        self.transition_stride=transition_stride # transition stride for each stage\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm' or 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        self.device=device\n",
    "        self.num_stages = len(num_blocks)\n",
    "        \n",
    "        self.stem = Stem(stem_in_channels=self.input_channels, stem_out_channels=main_channels[0], stem_kernel_sz=self.stem_kersz, stem_stride=self.stem_stride,\n",
    "                         stem_padding=adjust_padding_for_strided_output(self.stem_kersz, self.stem_stride), stem_dilation=1, stem_groups=1, device=self.device)\n",
    "        self.stages = []\n",
    "        for i in range(self.num_stages):\n",
    "            if i == 0:\n",
    "                self.stages.append(Stage(transition_flag=False, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                    kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                    drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, norm_mode=self.norm_mode, device='cuda'))\n",
    "            else:\n",
    "                self.stages.append(Stage(transition_flag=True, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                        kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                        drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, transition_channels=self.main_channels[i-1], transition_kersz=self.transition_kersz[i],\n",
    "                                        transition_stride=self.transition_stride[i], norm_mode=self.norm_mode, device='cuda'))\n",
    "        self.stages = torch.nn.ModuleList(self.stages)\n",
    "    \n",
    "    # as same as u-net encoder design, each stage outputs final block output in each stage\n",
    "    def forward(self, x):\n",
    "        stage_output = []\n",
    "        x = self.stem(x)\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.stages[i](x)\n",
    "            stage_output.append(x)\n",
    "        return stage_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each stage output 0:  torch.Size([1, 48, 56, 168])\n",
      "each stage output 1:  torch.Size([1, 96, 28, 84])\n",
      "each stage output 2:  torch.Size([1, 192, 14, 42])\n",
      "each stage output 3:  torch.Size([1, 384, 7, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "DetectorBackbone                                   [1, 48, 56, 168]          --\n",
       "├─Stem: 1-1                                        [1, 48, 56, 168]          --\n",
       "│    └─Conv2d: 2-1                                 [1, 48, 56, 168]          2,352\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─Stage: 2-2                                  [1, 48, 56, 168]          --\n",
       "│    │    └─ModuleList: 3-1                        --                        2,780,793\n",
       "│    └─Stage: 2-3                                  [1, 96, 28, 84]           --\n",
       "│    │    └─ModuleList: 3-2                        --                        1,621,170\n",
       "│    └─Stage: 2-4                                  [1, 192, 14, 42]          --\n",
       "│    │    └─ModuleList: 3-3                        --                        5,153,580\n",
       "│    └─Stage: 2-5                                  [1, 384, 7, 21]           --\n",
       "│    │    └─ModuleList: 3-4                        --                        4,431,816\n",
       "====================================================================================================\n",
       "Total params: 13,989,711\n",
       "Trainable params: 13,989,711\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.71\n",
       "====================================================================================================\n",
       "Input size (MB): 1.81\n",
       "Forward/backward pass size (MB): 211.14\n",
       "Params size (MB): 55.96\n",
       "Estimated Total Size (MB): 268.91\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_list, dp_mode = create_linear_p([3,3,9,3], 'batch', 0.25) # creates linearly decaying stochastic depth drop rate and mode\n",
    "\n",
    "detector_encoder = DetectorBackbone(num_blocks=[3,3,9,3], input_channels=3, stem_kersz=(4,4), stem_stride=(4,4), \n",
    "                                   img_hw=[(56, 168), (28, 84), (14, 42), (7,21)], main_channels=[48, 96, 192, 384], expansion_dim=[48*4, 96*4, 192*4, 384*4], \n",
    "                                   kernel_sz=[(7,7)]*4, stride=[(1,1)]*4, padding=['same']*4, dilation=[1]*4, groups=[1]*4, droprate=dp_list, drop_mode=dp_mode, \n",
    "                                   use_se=[True]*4, squeeze_ratio=16, transition_kersz=[-1, (1,1), (1,1), (1,1)], transition_stride=[-1, (2,2), (2,2), (2,2)], norm_mode='layer_norm', device='cuda')\n",
    "\n",
    "# fpn has the structure that goes through 0 to 3 stage output, we can use corresponding (28, 84), (14, 42), (7, 21) stage output,\n",
    "# or we can use all stages output to constructure fpn structure.\n",
    "backbone_o = detector_encoder(ex_img)\n",
    "for i, stage in enumerate(backbone_o):\n",
    "    print(f\"each stage output {i}: \", stage.shape)\n",
    "\n",
    "torchinfo.summary(detector_encoder, (1, 3, 224, 672))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled p4 shape:  torch.Size([1, 1, 14, 42])\n",
      "upsampeld p3 shape:  torch.Size([1, 1, 28, 84])\n",
      "upsampled p2 shape:  torch.Size([1, 1, 56, 168])\n"
     ]
    }
   ],
   "source": [
    "# transposed convolution upsample scheme!\n",
    "P4 = backbone_o[3] # (384, 7, 21)\n",
    "P3 = backbone_o[2] # (192, 14, 42)\n",
    "P2 = backbone_o[1] # (96, 28, 84)\n",
    "P1 = backbone_o[0] # (48, 56, 168)\n",
    "\n",
    "upsample_p4_3 = torch.nn.ConvTranspose2d(in_channels=384, out_channels=1, kernel_size=(7,7), stride=(2,2), padding=(3,3), output_padding=(1,1), groups=1, dilation=1, device='cuda')\n",
    "upsampled_p4 = upsample_p4_3(P4)\n",
    "print(\"upsampled p4 shape: \", upsampled_p4.shape)\n",
    "\n",
    "upsample_p3_2 = torch.nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(7,7), stride=(2,2), padding=(3,3), output_padding=(1,1), groups=1, dilation=1, device='cuda')\n",
    "upsampled_p3 = upsample_p3_2(upsampled_p4)\n",
    "print(\"upsampeld p3 shape: \", upsampled_p3.shape)\n",
    "\n",
    "upsample_p2_1 = torch.nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(7,7), stride=(2,2), padding=(3,3), output_padding=(1,1), groups=1, dilation=1, device='cuda')\n",
    "upsampled_p2 = upsample_p2_1(upsampled_p3)\n",
    "print(\"upsampled p2 shape: \", upsampled_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPN: Feature Pyramid Network (hierarchical structure of each feature output from each backbone stage)\n",
    "# Input: Output from DetectorBackbone forward()\n",
    "# Output: Feature Structure constructed from Input. Information from all stage output are reinforced by top<->down feature sharing\n",
    "# This Module will be connected to head of detector which outputs the bounding box location and classifying result.\n",
    "class FPN(torch.nn.Module):\n",
    "    def __init__(self, nstage_to_use, bidirectional, in_channels, out_channels, img_hw, downsample_kersz, upsample_kersz, downsample_stride, upsample_stride, \n",
    "                 downsample_dilation, upsample_dilation, downsample_groups, upsample_groups, upsample_padding, upsample_out_padding, norm_mode, device, dtype):\n",
    "        super().__init__()\n",
    "        # all list are orderd as low to high feature layer (down - up ordering) (example: [(56, 168),(28,84),(14,42),(7,21)])\n",
    "        self.nstage_to_use = nstage_to_use # number of stages in FPN structure\n",
    "        self.bidirectional = bidirectional # whether to share information bidirectional way (top->down, down->top)\n",
    "        self.in_channels = in_channels # number of chanels from DetectorBackbone output #(example: [48, 96, 192, 384])\n",
    "        self.out_channels = out_channels # number of channels that FPN outputs\n",
    "        self.img_hw = img_hw # list of image (h, w) shape, this parameter is only used when norm_mode=='layer_norm' otherwise it notifies the user how the shape of input tensors are changed.\n",
    "        \n",
    "        self.downsample_kersz = downsample_kersz # kernel size used for top-down transform convolution\n",
    "        self.upsample_kersz = upsample_kersz # kernel size used for down-top transform transposed convolution\n",
    "        self.downsample_stride = downsample_stride # stride used for top-down transform convolution (mostly 2)\n",
    "        self.upsample_stride = upsample_stride # stride size used for down-top transform transposed convolution\n",
    "        \n",
    "        self.downsample_dilation = downsample_dilation # dilation used for top-down transform convolution\n",
    "        self.upsample_dilation = upsample_dilation # dilation used for down-top transform transposed convolution\n",
    "        self.downsample_groups = downsample_groups # number of groups used for top-down transform convolution\n",
    "        self.upsample_groups = upsample_groups # number of groups used for down-top transform transposed convolution\n",
    "        \n",
    "        self.upsample_padding = upsample_padding # padding used for down-top transform transposed convolution\n",
    "        self.upsample_out_padding = upsample_out_padding # output padding used for down-top transform transposed convolution (this is used for adjusting output shape of transposed convolution)\n",
    "        \n",
    "        if norm_mode not in ['batch_norm', 'layer_norm']:\n",
    "            raise Exception('norm_mode must be one of \"batch_norm\" or \"layer_norm\"')\n",
    "        \n",
    "        self.norm_mode = norm_mode\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        if self.nstage_to_use > len(self.in_channels) or self.nstage_to_use <= 0:\n",
    "            raise Exception(\"number of stage to use must be equal or smaller than total number of stages. Also it must be bigger than 0\")\n",
    "        \n",
    "        # number of transform must be equal to nstage_to_use-1 (transfrom applies to intervals of each stages)\n",
    "        if self.nstage_to_use-1 != len(self.downsample_kersz):\n",
    "            raise Exception(\"number of transforms must be self.nstage_to_use-1!\")\n",
    "        \n",
    "        self.num_stages = len(self.in_channels) # total number of channels\n",
    "        \n",
    "        # only bidirection FPN use down to up convolution\n",
    "        if self.bidirectional:\n",
    "            self.down_up_transform = []\n",
    "            # if nstage_to_use is not same as num_stages it means that lower feature pyramid is excluded\n",
    "            # for example, if nstage_to_use=3, num_stages=4, we will use top-3 stages (top 3 small feature map) to consturcture fpn. // inputs: (0,1,2,3) -> outputs: (1,2,3)\n",
    "            for i, stage_i in enumerate(range(self.num_stages-self.nstage_to_use, self.num_stages-1), 0): \n",
    "                self.down_up_transform.append(torch.nn.Conv2d(in_channels=self.in_channels[stage_i], out_channels=self.in_channels[stage_i+1], kernel_size=self.downsample_kersz[i], \n",
    "                                                        stride=self.downsample_stride[i], padding=adjust_padding_for_strided_output(self.downsample_kersz[i], self.downsample_stride[i]),\n",
    "                                                            dilation=self.downsample_dilation[i], groups=self.downsample_groups[i], device=self.device, dtype=self.dtype))\n",
    "            self.down_up_transform = torch.nn.ModuleList(self.down_up_transform)\n",
    "\n",
    "        self.up_down_transform = []\n",
    "        for i, stage_i in enumerate(range(self.num_stages-1, self.num_stages-self.nstage_to_use, -1), 0):\n",
    "            self.up_down_transform.append(torch.nn.ConvTranspose2d(in_channels=self.in_channels[stage_i], out_channels=self.in_channels[stage_i-1], kernel_size=self.upsample_kersz[i],\n",
    "                                                              stride=self.upsample_stride[i], padding=self.upsample_padding[i], output_padding=self.upsample_out_padding[i],\n",
    "                                                              groups=self.upsample_groups[i], dilation=self.upsample_dilation[i], device=self.device, dtype=self.dtype))\n",
    "        self.up_down_transform = torch.nn.ModuleList(self.up_down_transform)\n",
    "        \n",
    "        # number of downsampled_output and upsampled_output must be sample\n",
    "        if self.bidirectional and len(self.down_up_transform) != len(self.up_down_transform):\n",
    "            raise Exception(\"number of down_up_transform and up_down_transform must be same!\")\n",
    "        \n",
    "        self.post_conv = []\n",
    "        self.normalize = []\n",
    "        # transform of post_processing is pointwise (1x1) convolution. this transforms adjust the number of channels of each image to self.out_channels\n",
    "        # through pointwise convolution, normalization method (layer_norm or batch_norm) is followed to increase the stability of training.\n",
    "        for stage_i in range(self.num_stages-1, self.num_stages-self.nstage_to_use-1, -1):\n",
    "            self.post_conv.append(torch.nn.Conv2d(in_channels=self.in_channels[stage_i], out_channels=self.out_channels, kernel_size=(1,1),\n",
    "                                                  stride=(1,1), padding=(0,0), dilation=1, groups=1, device=self.device, dtype=self.dtype))\n",
    "            if self.norm_mode=='layer_norm':\n",
    "                self.normalize.append(torch.nn.LayerNorm(normalized_shape=[self.out_channels, self.img_hw[stage_i][0], self.img_hw[stage_i][1]], device=self.device, dtype=self.dtype))\n",
    "            elif self.norm_mode=='batch_norm':\n",
    "                self.normalize.append(torch.nn.BatchNorm2d(num_features=self.out_channels, device=self.device, dtype=self.dtype))\n",
    "\n",
    "        self.post_conv = torch.nn.ModuleList(self.post_conv)\n",
    "        self.normalize = torch.nn.ModuleList(self.normalize)\n",
    "        \n",
    "    \n",
    "    # fpn inputs stage output of backbone and constructure fpn structure\n",
    "    def forward(self, backbone_stage_o): # backbone_stage_o: (example: [(56, 168),(28,84),(14,42),(7,21)])\n",
    "        fpn_o = [] # output of fpn module must output nstage_to_use number of features\n",
    "        \n",
    "        for i, stage_i in enumerate(range(self.num_stages-1, self.num_stages-self.nstage_to_use, -1), 0):\n",
    "            if i==0:\n",
    "                fpn_o.append(backbone_stage_o[stage_i])\n",
    "            x = self.up_down_transform[i](backbone_stage_o[stage_i]) # [(7,21)->(14,42), (14,42)->(28,84), (28,84)->(56,168)]\n",
    "            x = x + backbone_stage_o[stage_i-1]\n",
    "            fpn_o.append(x)\n",
    "        \n",
    "        # only bidirectional fpn uses down-to-up (common conv2d) transforms\n",
    "        if self.bidirectional:\n",
    "            #fpn_o: [(7,21), (14,42), (28,84), (56,168)]\n",
    "            for i, stage_i in enumerate(range(self.num_stages-self.nstage_to_use, self.num_stages-1), 0):\n",
    "                x = self.down_up_transform[i](x) # [(56,168)->(28,84), (28,84)->(14,42), (14,42)->(7,21)]\n",
    "                x = x + fpn_o[self.nstage_to_use-2-i]\n",
    "                fpn_o[self.nstage_to_use-2-i] = x\n",
    "        \n",
    "        for i in range(self.nstage_to_use):\n",
    "            fpn_o[i] = self.post_conv[i](fpn_o[i])\n",
    "            fpn_o[i] = self.normalize[i](fpn_o[i])\n",
    "            \n",
    "        # output of fpn module are reinfored by top-down, down-top feature construction \n",
    "        # ex) ordering of modules is top down: [(7, 21), (14,42), (28, 84), (56, 168)]\n",
    "        return fpn_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th fpn output shape:  torch.Size([1, 224, 7, 21])\n",
      "1th fpn output shape:  torch.Size([1, 224, 14, 42])\n",
      "2th fpn output shape:  torch.Size([1, 224, 28, 84])\n",
      "3th fpn output shape:  torch.Size([1, 224, 56, 168])\n",
      "\n",
      "0th fpn output shape:  torch.Size([1, 224, 7, 21])\n",
      "1th fpn output shape:  torch.Size([1, 224, 14, 42])\n",
      "2th fpn output shape:  torch.Size([1, 224, 28, 84])\n",
      "\n",
      "0th fpn output shape:  torch.Size([1, 224, 7, 21])\n",
      "1th fpn output shape:  torch.Size([1, 224, 14, 42])\n",
      "2th fpn output shape:  torch.Size([1, 224, 28, 84])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# class FPN(torch.nn.ModuleList):\n",
    "#     def __init__(self, nstage_to_use, bidirectional, in_channels, out_channels, img_hw, downsample_kersz, upsample_kersz, downsample_stride, upsample_stride, \n",
    "#                  downsample_dilation, upsample_dilation, downsample_groups, upsample_groups, upsample_padding, upsample_out_padding, norm_mode, device, dtype):\n",
    "\n",
    "fpn = FPN(nstage_to_use=4, bidirectional=True, in_channels=[48, 96, 192, 384], out_channels=224, img_hw=[(56, 168), (28, 84), (14, 42), (7, 21)],\n",
    "          downsample_kersz=[(7,7), (7,7), (7,7)], upsample_kersz=[(7,7), (7,7), (7,7)], downsample_stride=[(2,2), (2,2), (2,2)], upsample_stride=[(2,2), (2,2), (2,2)],\n",
    "          downsample_dilation=[1,1,1], upsample_dilation=[1,1,1], downsample_groups=[1,1,1], upsample_groups=[1,1,1], \n",
    "          upsample_padding=[(3,3), (3,3), (3,3)], upsample_out_padding=[(1,1), (1,1), (1,1)], norm_mode='layer_norm', device='cuda', dtype=None)\n",
    "fpn_output = fpn(backbone_o)\n",
    "\n",
    "for i in range(len(fpn_output)):\n",
    "    print(f\"{i}th fpn output shape: \", fpn_output[i].shape)\n",
    "\n",
    "\n",
    "fpn = FPN(nstage_to_use=3, bidirectional=True, in_channels=[48, 96, 192, 384], out_channels=224, img_hw=[(56, 168), (28, 84), (14, 42), (7, 21)],\n",
    "          downsample_kersz=[(7,7), (7,7)], upsample_kersz=[(7,7), (7,7)], downsample_stride=[(2,2), (2,2)], upsample_stride=[(2,2), (2,2)],\n",
    "          downsample_dilation=[1,1], upsample_dilation=[1,1], downsample_groups=[1,1], upsample_groups=[1,1], \n",
    "          upsample_padding=[(3,3), (3,3)], upsample_out_padding=[(1,1), (1,1)], norm_mode='layer_norm', device='cuda', dtype=None)\n",
    "\n",
    "print()\n",
    "fpn_output = fpn(backbone_o)\n",
    "for i in range(len(fpn_output)):\n",
    "    print(f\"{i}th fpn output shape: \", fpn_output[i].shape)\n",
    "    \n",
    "\n",
    "fpn = FPN(nstage_to_use=3, bidirectional=False, in_channels=[48, 96, 192, 384], out_channels=224, img_hw=[(56, 168), (28, 84), (14, 42), (7, 21)],\n",
    "          downsample_kersz=[(7,7), (7,7)], upsample_kersz=[(7,7), (7,7)], downsample_stride=[(2,2), (2,2)], upsample_stride=[(2,2), (2,2)],\n",
    "          downsample_dilation=[1,1], upsample_dilation=[1,1], downsample_groups=[1,1], upsample_groups=[1,1], \n",
    "          upsample_padding=[(3,3), (3,3)], upsample_out_padding=[(1,1), (1,1)], norm_mode='layer_norm', device='cuda', dtype=None)\n",
    "\n",
    "print()\n",
    "fpn_output = fpn(backbone_o)\n",
    "for i in range(len(fpn_output)):\n",
    "    print(f\"{i}th fpn output shape: \", fpn_output[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
