{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import torchinfo\n",
    "import albumentations\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "drwxr-xr-x 2 root root  4096 Jan 13 10:26 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r--r-- 1 root root 31357 Jan 15 05:10 convnext_se.ipynb\n",
      "-rw-r--r-- 1 root root 19119 Jan 13 10:26 convnext_se.py\n"
     ]
    }
   ],
   "source": [
    "%ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of validation:  50000\n"
     ]
    }
   ],
   "source": [
    "# list all the images in the image\n",
    "IMG_PATH = '/work/dataset/imagenet_1k/ILSVRC/Data/CLS-LOC/'\n",
    "train_set = glob.glob(IMG_PATH+'train/*/*')\n",
    "val_set = glob.glob(IMG_PATH+'val/*')\n",
    "\n",
    "print(\"size of validation: \", len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_image = Image.open(val_set[0])\n",
    "ex_image = ex_image.resize([224,224])\n",
    "tensor_img = torch.tensor(np.asarray(ex_image)).unsqueeze(0).permute(0, 3, 1, 2).float().to('cuda')\n",
    "tensor_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_padding_for_strided_output(kernel_size, stride):\n",
    "        # range of padding: (kernel_size - stride)/2 <= padding < (kernel_size)/2\n",
    "        # this ensures that output convolution channel size are 1/(transition_stride) of the input size\n",
    "        bottom_inclusive_h = (kernel_size[0] -stride[0])/2\n",
    "        ceil_exclusive_h = (kernel_size[0])/2\n",
    "        \n",
    "        bottom_inclusive_w = (kernel_size[1] - stride[1])/2\n",
    "        ceil_exclusive_w = (kernel_size[1])/2\n",
    "        \n",
    "        padding_h = -1\n",
    "        padding_w = -1\n",
    "        \n",
    "        nearest_nat = np.floor(bottom_inclusive_h)\n",
    "        if nearest_nat == bottom_inclusive_h: # in case of (1 <= p < 1.5) -> p == np.rint(1)\n",
    "            padding_h = int(nearest_nat)\n",
    "        else:\n",
    "            if nearest_nat < bottom_inclusive_h: # in case of (1.5 <= p < 2.5) -> p == np.rint(1.5)+1\n",
    "                padding_h = int(nearest_nat+1)\n",
    "            else:\n",
    "                raise Exception(\"Invalid transition stride and kernel height size. Check if kernel_size >= stride.\")\n",
    "        \n",
    "        nearest_nat = np.floor(bottom_inclusive_w)\n",
    "        if nearest_nat == bottom_inclusive_w: # in case of (1 <= p < 1.5) -> p == np.rint(1)\n",
    "            padding_w = int(nearest_nat)\n",
    "        else:\n",
    "            if nearest_nat < bottom_inclusive_w: # in case of (1.5 <= p < 2.5) -> p == np.rint(1.5)+1\n",
    "                padding_w = int(nearest_nat + 1)\n",
    "            else:\n",
    "                raise Exception(\"Invalid transition stride and kernel width size. Check if kernel_size >= stride.\")\n",
    "        return (padding_h, padding_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseSepConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sz, stride, padding, dilation, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.depthwise_inc = in_channels\n",
    "        self.depthwise_ouc = in_channels\n",
    "        self.depthwise_kersz = kernel_sz\n",
    "        self.depthwise_std = stride\n",
    "        self.depthwise_pad = padding\n",
    "        self.depthwise_dil = dilation\n",
    "        self.depthwise_grp = in_channels\n",
    "        \n",
    "        self.pointwise_inc = in_channels\n",
    "        self.pointwise_ouc = out_channels\n",
    "        self.pointwise_kersz = (1,1)\n",
    "        self.pointwise_std = (1,1)\n",
    "        self.pointwise_pad = 0\n",
    "        self.pointwise_dil = 1\n",
    "        self.pointwise_grp = 1\n",
    "        \n",
    "        self.device = device\n",
    "        self.depthwise_conv = torch.nn.Conv2d(in_channels=self.depthwise_inc, out_channels=self.depthwise_ouc, kernel_size=self.depthwise_kersz,\n",
    "                                              stride=self.depthwise_std, padding=self.depthwise_pad, dilation=self.depthwise_dil, groups=self.depthwise_grp, bias=True, padding_mode='zeros',\n",
    "                                              device=self.device)\n",
    "        self.pointwise_conv = torch.nn.Conv2d(in_channels=self.pointwise_inc, out_channels=self.pointwise_ouc, kernel_size=self.pointwise_kersz, stride=self.pointwise_std,\n",
    "                                              padding=self.pointwise_pad, dilation=self.pointwise_dil, groups=self.pointwise_grp, bias=True, padding_mode='zeros',\n",
    "                                              device=self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depthwise_output shape:  torch.Size([1, 64, 56, 56])\n",
      "is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "depthwise_conv = DepthWiseSepConv(3, 64, (4,4), (4,4), (0,0), 1, 'cuda')\n",
    "depthwise_output = depthwise_conv(tensor_img)\n",
    "print(\"depthwise_output shape: \", depthwise_output.shape)\n",
    "print(\"is gradient alive?: \", depthwise_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNextV1: use 4x4 kernel size, with stride 4 (Non-overlapping way) -> 224x224 -> 56x56\n",
    "# things to consider: we use standard 2d convolution in stem layer\n",
    "# compared to original resnet, this corresponds to stage1, stage2_1 layer\n",
    "class Stem(torch.nn.Module):\n",
    "    def __init__(self, stem_in_channels, stem_out_channels, stem_kernel_sz, stem_stride, stem_padding, stem_dilation, stem_groups, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.in_channels = stem_in_channels\n",
    "        self.out_channels = stem_out_channels\n",
    "        \n",
    "        self.kernel_sz = stem_kernel_sz\n",
    "        self.stride = stem_stride\n",
    "        self.padding = stem_padding\n",
    "        \n",
    "        self.dilation = stem_dilation\n",
    "        self.groups = stem_groups\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.stem_conv = torch.nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_sz, stride=self.stride, padding=self.padding,\n",
    "                                         dilation=self.dilation, groups=self.groups, bias=True, padding_mode='zeros', device=self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem_output shape:  torch.Size([1, 64, 56, 56])\n",
      "is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "stem = Stem(3, 64, (4,4), (4,4), stem_padding=(0,0), stem_dilation=1, stem_groups=1, device='cuda')\n",
    "stem_output = stem(tensor_img)\n",
    "print(\"stem_output shape: \", stem_output.shape)\n",
    "print(\"is gradient alive?: \", stem_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel-wise multiplication is guaranteed        \n",
    "# x = torch.ones(1, 3, 24, 24)\n",
    "# m = torch.tensor([1, 2, 4])\n",
    "# m = m.reshape((1, 3, 1, 1))\n",
    "# s = x * m\n",
    "        \n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, img_hw, in_channels, out_channels, kernel_sz, stride, padding, dilation, groups, droprate, drop_mode, \n",
    "                 use_se:bool, squeeze_ratio, transition=False, transition_channels=-1, transition_kersz=-1, transition_stride=-1,\n",
    "                 norm_mode='layer_norm', device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_h = img_hw[0]\n",
    "        self.img_w = img_hw[1]\n",
    "        \n",
    "        self.block_channels = in_channels\n",
    "        self.block_out_channels = out_channels\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation \n",
    "        self.groups = groups # this controls the number of groups for pointwise convolution\n",
    "        \n",
    "        self.droprate = droprate # stochastic depth drop rate\n",
    "        if drop_mode not in ['row', 'batch']:\n",
    "            raise Exception(\"drop_mode must be 'row' or 'batch'\")\n",
    "        self.drop_mode = drop_mode\n",
    "        \n",
    "        self.use_se = use_se # whether to use se operation in the block\n",
    "        self.squeeze_r = squeeze_ratio\n",
    "        \n",
    "        if norm_mode not in ['batch_norm', 'layer_norm']:\n",
    "            raise Exception(f\"Unsupported Normaliztion method: {norm_mode}. Normalization method must be either 'batch_norm', 'layer_norm'.\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        self.device = device\n",
    "        self.transition = transition\n",
    "        \n",
    "        self.transition_channels = transition_channels # number of last channels before transitioning\n",
    "        self.transition_kersz = transition_kersz # kernel size for transitioning\n",
    "        self.transition_stride = transition_stride # number of strides for shape matching in transitioning process\n",
    "        \n",
    "        if self.transition:\n",
    "            self.depthwisesep_conv = DepthWiseSepConv(in_channels=self.transition_channels, out_channels=self.block_channels,\n",
    "                                                    kernel_sz=self.kernel_sz, stride=self.transition_stride, padding=adjust_padding_for_strided_output(self.kernel_sz, self.transition_stride), dilation=1, device=self.device)\n",
    "            self.transition_conv = torch.nn.Conv2d(in_channels=self.transition_channels, out_channels=self.block_channels, kernel_size=self.transition_kersz, stride=self.transition_stride, \n",
    "                                                    padding=adjust_padding_for_strided_output(self.transition_kersz, self.transition_stride), dilation=1, groups=1, device=self.device)\n",
    "        else:\n",
    "            self.depthwisesep_conv = DepthWiseSepConv(in_channels=self.block_channels, out_channels=self.block_channels, kernel_sz=self.kernel_sz, stride=self.stride, padding=self.padding, dilation=self.dilation, device=self.device)\n",
    "        self.pointwise_1 = torch.nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_out_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=self.groups, device=self.device)\n",
    "        self.pointwise_2 = torch.nn.Conv2d(in_channels=self.block_out_channels, out_channels=self.block_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=self.groups, device=self.device)\n",
    "        \n",
    "        if self.use_se:\n",
    "            self.fc1 = torch.nn.Conv2d(in_channels=self.block_channels, out_channels=int(self.block_channels/self.squeeze_r), kernel_size=(1,1), stride=(1,1), padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=self.device)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Conv2d(in_channels=int(self.block_channels/self.squeeze_r), out_channels=self.block_channels, kernel_size=(1,1), stride=(1,1), padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=self.device)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        # use batch normalization when dealing with large input size\n",
    "        if self.norm_mode == 'layer_norm':\n",
    "            self.normalization = torch.nn.LayerNorm([self.block_channels, self.img_h, self.img_w], device=self.device)\n",
    "        elif self.norm_mode == 'batch_norm':\n",
    "            self.normalization = torch.nn.BatchNorm2d(num_features=self.block_channels, device=self.device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        highway = x\n",
    "        \n",
    "        x = self.depthwisesep_conv(x)\n",
    "        x = self.normalization(x)\n",
    "        x = self.pointwise_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.pointwise_2(x)\n",
    "        \n",
    "        if self.use_se:\n",
    "            squeeze = torch.nn.functional.avg_pool2d(x, x.size()[2:]) # same as global average pooling\n",
    "            excitation = self.fc1(squeeze)\n",
    "            excitation = self.relu(excitation)\n",
    "            excitation = self.fc2(excitation)\n",
    "            attention = self.sigmoid(excitation) # (B, C, 1, 1)\n",
    "            x = x * attention\n",
    "            \n",
    "        if self.transition: # if it is in transitioning process\n",
    "            highway = self.transition_conv(highway)\n",
    "        \n",
    "        highway = highway + torchvision.ops.stochastic_depth(x, p=self.droprate, mode=self.drop_mode, training=self.training) # stochastic depth drop for residuals\n",
    "        return highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_output shape:  torch.Size([1, 64, 56, 56])\n",
      "block1: is gradient alive?:  True\n",
      "block2_output shape:  torch.Size([1, 128, 28, 28])\n",
      "block2: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "block1 = Block([56, 56], in_channels=64, out_channels=64, kernel_sz=(7,7), stride=(1,1), padding='same', dilation=1, groups=1, droprate=0.1, drop_mode='batch', use_se=True, squeeze_ratio=16, norm_mode='layer_norm', device='cuda')\n",
    "block1_output = block1(stem_output)\n",
    "print(\"block1_output shape: \", block1_output.shape)\n",
    "print(\"block1: is gradient alive?: \", block1_output.requires_grad)\n",
    "\n",
    "block2 = Block([28, 28], in_channels=128, out_channels=256, kernel_sz=(7,7), stride=(1,1), padding='same', dilation=1, groups=1, droprate=0.1, drop_mode='batch', use_se=True, squeeze_ratio=16, transition=True, transition_channels=64, transition_kersz=(3,3), transition_stride=(2,2), norm_mode='layer_norm', device='cuda')\n",
    "block2_output = block2(block1_output)\n",
    "print(\"block2_output shape: \", block2_output.shape)\n",
    "print(\"block2: is gradient alive?: \", block2_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage(torch.nn.Module):\n",
    "    def __init__(self, transition_flag, num_blocks:int, img_hw:list, in_channels, out_channels, kernel_sz, stride, padding, dilation, groups, droprate:list, drop_mode:list, use_se:bool, squeeze_ratio:int, \n",
    "                 transition_channels=-1, transition_kersz=-1, transition_stride=-1, norm_mode='layer_norm', device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transition_flag = transition_flag # whether stage includes transition=True blocks\n",
    "        self.num_blocks = num_blocks\n",
    "        self.img_h = img_hw[0] # image height, width corresponding to specific stage\n",
    "        self.img_w = img_hw[1]\n",
    "        self.stage_channels = in_channels\n",
    "        self.stage_out_channels = out_channels\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.droprate=droprate\n",
    "        for i in range(len(drop_mode)):\n",
    "            if drop_mode[i] not in ['row', 'batch']:\n",
    "                raise Exception(\"drop_mode must be 'row' or 'batch'\")\n",
    "        self.drop_mode=drop_mode\n",
    "        self.use_se=use_se # whether to use se operation in the blocks\n",
    "        self.squeeze_ratio=squeeze_ratio\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm' or 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        self.device = device\n",
    "        \n",
    "        self.transition_channels = transition_channels\n",
    "        self.transition_kersz = transition_kersz\n",
    "        self.transition_stride = transition_stride\n",
    "        \n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0 and self.transition_flag: # the start block of stage process transitioning.\n",
    "                self.blocks.append(Block([self.img_h, self.img_w], in_channels=self.stage_channels, out_channels=self.stage_out_channels, kernel_sz=self.kernel_sz, stride=self.stride, padding=self.padding, dilation=self.dilation,\n",
    "                                            groups=self.groups, droprate=self.droprate[i], drop_mode=self.drop_mode[i], use_se=self.use_se, squeeze_ratio=self.squeeze_ratio, transition=True, transition_channels=self.transition_channels, \n",
    "                                            transition_kersz=self.transition_kersz, transition_stride=self.transition_stride, norm_mode=self.norm_mode, device=self.device))\n",
    "            else:\n",
    "                self.blocks.append(Block([self.img_h, self.img_w], in_channels=self.stage_channels, out_channels=self.stage_out_channels, kernel_sz=self.kernel_sz, stride=self.stride, padding=self.padding, dilation=self.dilation,\n",
    "                                            groups=self.groups, droprate=self.droprate[i], drop_mode=self.drop_mode[i], use_se=self.use_se, squeeze_ratio=self.squeeze_ratio, norm_mode=self.norm_mode, device=self.device))\n",
    "        self.blocks = torch.nn.ModuleList(self.blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage1_output shape:  torch.Size([1, 64, 56, 56])\n",
      "stage1_output: is gradient alive?:  True\n",
      "stage2_output shape:  torch.Size([1, 128, 28, 28])\n",
      "stage2_output: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "stage1 = Stage(False, 2, [56,56], in_channels=64, out_channels=256, kernel_sz=(7,7), stride=(1,1), padding='same', dilation=1, groups=1, droprate=[0.1, 0.1], drop_mode=['batch', 'batch'], use_se=True, squeeze_ratio=16, norm_mode='layer_norm', device='cuda')\n",
    "stage1_output = stage1(stem_output)\n",
    "print(\"stage1_output shape: \", stage1_output.shape)\n",
    "print(\"stage1_output: is gradient alive?: \", stage1_output.requires_grad)\n",
    "\n",
    "stage2 = Stage(True, 2, [28, 28], in_channels=128, out_channels=512, kernel_sz=(7,7), stride=(1,1), padding='same', dilation=1, groups=1, droprate=[0.1, 0.1], drop_mode=['batch', 'batch'], use_se=False, squeeze_ratio=16, \n",
    "               transition_channels=64, transition_kersz=(3,3), transition_stride=(2,2), norm_mode='layer_norm', device='cuda')\n",
    "stage2_output = stage2(stage1_output)\n",
    "print(\"stage2_output shape: \", stage2_output.shape)\n",
    "print(\"stage2_output: is gradient alive?: \", stage2_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextV1(torch.nn.Module):\n",
    "    def __init__(self, num_blocks:list, input_channels:int, stem_kersz:tuple, stem_stride:tuple, \n",
    "                 img_hw:list, main_channels:list, expansion_dim:list, kernel_sz:list, stride:list, padding:list, dilation:list, groups:list, droprate:list, drop_mode:list, use_se:list, squeeze_ratio:int,\n",
    "                 transition_kersz:list, transition_stride:list, norm_mode:str, device='cuda'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks # number of blocks for each stage\n",
    "        self.input_channels = input_channels # mostly RGB 3 channel images\n",
    "        self.stem_kersz = stem_kersz # kernel size for stem layer\n",
    "        self.stem_stride = stem_stride # stride for stem layer\n",
    "        self.img_hw = img_hw # representative image height and width for each stage\n",
    "        self.main_channels = main_channels # main channels for each stage\n",
    "        self.expansion_dim = expansion_dim # expansion dimension for each stage\n",
    "        self.kernel_sz = kernel_sz # kernel size for each stage\n",
    "        self.stride=stride # stride for each stage\n",
    "        self.padding=padding # padding for each stage\n",
    "        self.dilation=dilation # dilation for each stage\n",
    "        self.groups=groups # number of groups for each stage\n",
    "        self.droprate=droprate # constant droprate for all stage\n",
    "        self.drop_mode=drop_mode # drop_mode is same for all stage\n",
    "        self.use_se=use_se # flag for using se operation in each stage\n",
    "        self.squeeze_ratio=squeeze_ratio # squeeze_ratio is same for all stage\n",
    "        self.transition_kersz=transition_kersz # transition kernel size for each stage\n",
    "        self.transition_stride=transition_stride # transition stride for each stage\n",
    "        \n",
    "        if norm_mode not in ['batch_norm', 'layer_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm' or 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode # normalization mode (layer_norm: torch.nn.LayerNorm, batch_norm: torch.nn.BatchNorm2d)\n",
    "        \n",
    "        self.device=device\n",
    "        self.num_stages = len(num_blocks)\n",
    "        \n",
    "        self.stem = Stem(stem_in_channels=self.input_channels, stem_out_channels=main_channels[0], stem_kernel_sz=self.stem_kersz, stem_stride=self.stem_stride,\n",
    "                         stem_padding=adjust_padding_for_strided_output(self.stem_kersz, self.stem_stride), stem_dilation=1, stem_groups=1, device=self.device)\n",
    "        self.stages = []\n",
    "        for i in range(self.num_stages):\n",
    "            if i == 0:\n",
    "                self.stages.append(Stage(transition_flag=False, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                    kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                    drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, norm_mode=self.norm_mode, device='cuda'))\n",
    "            else:\n",
    "                self.stages.append(Stage(transition_flag=True, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                        kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                        drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, transition_channels=self.main_channels[i-1], transition_kersz=self.transition_kersz[i],\n",
    "                                        transition_stride=self.transition_stride[i], norm_mode=self.norm_mode, device='cuda'))\n",
    "        self.stages = torch.nn.ModuleList(self.stages)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.stages[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ConvNextV1                                         [1, 768, 7, 7]            --\n",
       "├─Stem: 1-1                                        [1, 96, 56, 56]           --\n",
       "│    └─Conv2d: 2-1                                 [1, 96, 56, 56]           4,704\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─Stage: 2-2                                  [1, 96, 56, 56]           --\n",
       "│    │    └─ModuleList: 3-1                        --                        2,071,296\n",
       "│    └─Stage: 2-3                                  [1, 192, 28, 28]          --\n",
       "│    │    └─ModuleList: 3-2                        --                        1,926,144\n",
       "│    └─Stage: 2-4                                  [1, 384, 14, 14]          --\n",
       "│    │    └─ModuleList: 3-3                        --                        13,483,008\n",
       "│    └─Stage: 2-5                                  [1, 768, 7, 7]            --\n",
       "│    │    └─ModuleList: 3-4                        --                        16,261,632\n",
       "====================================================================================================\n",
       "Total params: 33,746,784\n",
       "Trainable params: 33,746,784\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.81\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 140.74\n",
       "Params size (MB): 134.99\n",
       "Estimated Total Size (MB): 276.33\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_linear_p(block_nlist:list, dp_mode:str, last_p:float):\n",
    "    \n",
    "    droprate = []  # probability of stochastic drop depth for all blocks in all stages\n",
    "    drop_mode = [] # mode of stochastic drop depth for all blocks in all stages\n",
    "    \n",
    "    l_count = 0\n",
    "    num_of_l = sum(block_nlist)\n",
    "    \n",
    "    for s_index, num_block in enumerate(block_nlist,0): # starts from stage 0\n",
    "        p_list = []\n",
    "        mode_list = []\n",
    "        for b_index in range(num_block): # starts from block 0\n",
    "            p = (l_count/num_of_l) * (1-last_p)\n",
    "            p_list.append(p)\n",
    "            mode_list.append(dp_mode)\n",
    "            l_count+=1\n",
    "        droprate.append(p_list)\n",
    "        drop_mode.append(mode_list)\n",
    "    return droprate, drop_mode\n",
    "\n",
    "# create uniform stochastic depth drop probability list, mode\n",
    "def create_uniform_p(block_nlist:list, dp_mode:str, uniform_p:int):\n",
    "    droprate = []\n",
    "    drop_mode = []\n",
    "    \n",
    "    for s_index, num_block in enumerate(block_nlist, 0):\n",
    "        p_list = []\n",
    "        mode_list = []\n",
    "        for b_index in range(num_block):\n",
    "            p_list.append(uniform_p)\n",
    "            mode_list.append(dp_mode)\n",
    "        droprate.append(p_list)\n",
    "        drop_mode.append(mode_list)\n",
    "\n",
    "    return droprate, drop_mode\n",
    "\n",
    "\n",
    "dp_list, dp_mode = create_linear_p([3,3,9,3], 'batch', 0.5) # creates linearly decaying stochastic depth drop probability\n",
    "# dp_list, dp_mode = create_uniform_p([3,3,9,3], 'batch', 0.001) # create constant stochastic depth drop probability\n",
    "    \n",
    "convnext_backbone = ConvNextV1(num_blocks=[3, 3, 9, 3], input_channels=3, stem_kersz=(4,4), stem_stride=(4,4), img_hw=[(56,56),(28,28),(14,14),(7,7)], main_channels=[96, 192, 384, 768], expansion_dim=[96*4, 192*4, 384*4, 768*4],\n",
    "                               kernel_sz=[(7,7), (7,7), (7,7), (7,7)], stride=[(1,1),(1,1),(1,1),(1,1)], padding=['same', 'same', 'same', 'same'], dilation=[1,1,1,1], groups=[1,1,1,1], droprate=dp_list, drop_mode=dp_mode,\n",
    "                               use_se=[False, False, False, False], squeeze_ratio=16, transition_kersz=[-1, (1,1),(1,1),(1,1)], transition_stride=[-1, (2,2), (2,2), (2,2)], \n",
    "                               norm_mode='layer_norm', device='cuda')\n",
    "\n",
    "torchinfo.summary(convnext_backbone, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone output shape:  torch.Size([1, 768, 7, 7])\n",
      "backbone output: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "backbone_output = convnext_backbone(tensor_img)\n",
    "print(\"backbone output shape: \", backbone_output.shape)\n",
    "print(\"backbone output: is gradient alive?: \", backbone_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextV1CLS(torch.nn.Module):\n",
    "    def __init__(self, backbone:ConvNextV1, num_cls=1000, output_mode='logits', device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone=backbone\n",
    "        self.num_cls=num_cls\n",
    "        if output_mode not in ['logits', 'activation']:\n",
    "            raise Exception(\"output_mode must be 'logits' or 'activation'\")\n",
    "        self.output_mode=output_mode\n",
    "        self.device=device\n",
    "        \n",
    "        self.cls_head = torch.nn.Linear(in_features=backbone.main_channels[-1], out_features=self.num_cls, device=self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.nn.functional.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.flatten(start_dim=1, end_dim=3)\n",
    "        x = self.cls_head(x)\n",
    "        if self.output_mode=='logits':\n",
    "            return x\n",
    "        elif self.output_mode=='probs':\n",
    "            return torch.nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction output shape:  torch.Size([1, 1000])\n",
      "prediction output: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "convnext_v1_tiny = ConvNextV1CLS(convnext_backbone, num_cls=1000, output_mode='logits', device='cuda')\n",
    "pred_output = convnext_v1_tiny(tensor_img)\n",
    "print(\"prediction output shape: \", pred_output.shape)\n",
    "print(\"prediction output: is gradient alive?: \", pred_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
