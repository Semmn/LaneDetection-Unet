{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('backbone')))) # to import file from under same-level directory\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('utils'))))\n",
    "\n",
    "from backbone.convnext_se.convnext_se import ConvNextV1, Stage, Stem\n",
    "from utils.conv_2d import adjust_padding_for_strided_output, DepthWiseSepConv\n",
    "from utils.stochastic_depth_drop import create_linear_p, create_uniform_p\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import torchinfo\n",
    "import albumentations\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearly decaying survival probability:  [[0.0, 0.027777777777777776, 0.05555555555555555], [0.08333333333333333, 0.1111111111111111, 0.1388888888888889], [0.16666666666666666, 0.19444444444444445, 0.2222222222222222, 0.25, 0.2777777777777778, 0.3055555555555556, 0.3333333333333333, 0.3611111111111111, 0.3888888888888889], [0.4166666666666667, 0.4444444444444444, 0.4722222222222222]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ConvNextV1                                         [1, 768, 7, 7]            --\n",
       "├─Stem: 1-1                                        [1, 96, 56, 56]           --\n",
       "│    └─Conv2d: 2-1                                 [1, 96, 56, 56]           4,704\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─Stage: 2-2                                  [1, 96, 56, 56]           --\n",
       "│    │    └─ModuleList: 3-1                        --                        2,075,058\n",
       "│    └─Stage: 2-3                                  [1, 192, 28, 28]          --\n",
       "│    │    └─ModuleList: 3-2                        --                        1,940,580\n",
       "│    └─Stage: 2-4                                  [1, 384, 14, 14]          --\n",
       "│    │    └─ModuleList: 3-3                        --                        13,652,568\n",
       "│    └─Stage: 2-5                                  [1, 768, 7, 7]            --\n",
       "│    │    └─ModuleList: 3-4                        --                        16,485,264\n",
       "====================================================================================================\n",
       "Total params: 34,158,174\n",
       "Trainable params: 34,158,174\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.82\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 140.80\n",
       "Params size (MB): 136.63\n",
       "Estimated Total Size (MB): 278.03\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make structure that encoder and decoder are connected stagewise, we need to re-implement modules.\n",
    "\n",
    "dp_list, dp_mode = create_linear_p([3,3,9,3], 'batch', 0.5) # creates linearly decaying stochastic depth drop probability\n",
    "# dp_list, dp_mode = create_uniform_p([3,3,9,3], 'batch', 0.001) # create constant stochastic depth drop probability\n",
    "print(\"linearly decaying survival probability: \", dp_list)\n",
    "    \n",
    "convnext_t = ConvNextV1(num_blocks=[3, 3, 9, 3], input_channels=3, stem_kersz=(4,4), stem_stride=(4,4), img_hw=[(56,56),(28,28),(14,14),(7,7)], main_channels=[96, 192, 384, 768], expansion_dim=[96*4, 192*4, 384*4, 768*4],\n",
    "                               kernel_sz=[(7,7), (7,7), (7,7), (7,7)], stride=[(1,1),(1,1),(1,1),(1,1)], padding=['same', 'same', 'same', 'same'], dilation=[1,1,1,1], groups=[1,1,1,1], droprate=dp_list, drop_mode=dp_mode,\n",
    "                               use_se=[True, True, True, True], squeeze_ratio=16, transition_kersz=[-1, (1,1),(1,1),(1,1)], transition_stride=[-1, (2,2), (2,2), (2,2)], \n",
    "                               norm_mode='layer_norm', device='cuda')\n",
    "\n",
    "torchinfo.summary(convnext_t, (1, 3, 224, 224)) # convnext tiny (33M vs 29M: more weight saving on batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image size:  (1640, 590)\n",
      "torch.Size([1, 3, 224, 672])\n"
     ]
    }
   ],
   "source": [
    "ex_img = Image.open(\"/work/dataset/CULane/driver_193_90frame/06051132_0638.MP4/00000.jpg\")\n",
    "print(\"original image size: \", ex_img.size)\n",
    "ex_img_resize = ex_img.resize([672, 224]) # mask and image will be resized to this size - format (W, H)\n",
    "ex_img = torch.tensor(np.array(ex_img_resize)).unsqueeze(0).permute(0, 3, 1, 2).float().to('cuda')\n",
    "print(ex_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnext v1 + sequeeze and excitation module + Unet Encoder (returns each stage output)\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_blocks:list, input_channels:int, stem_kersz:tuple, stem_stride:tuple, \n",
    "                 img_hw:list, main_channels:list, expansion_dim:list, kernel_sz:list, stride:list, padding:list, dilation:list, groups:list, droprate:list, drop_mode:list, use_se:list, squeeze_ratio:int,\n",
    "                 transition_kersz:list, transition_stride:list, norm_mode:str, device='cuda'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks # number of blocks for each stage\n",
    "        self.input_channels = input_channels # mostly RGB 3 channel images\n",
    "        self.stem_kersz = stem_kersz # kernel size for stem layer\n",
    "        self.stem_stride = stem_stride # stride for stem layer\n",
    "        self.img_hw = img_hw # representative image height and width for each stage\n",
    "        self.main_channels = main_channels # main channels for each stage\n",
    "        self.expansion_dim = expansion_dim # expansion dimension for each stage\n",
    "        self.kernel_sz = kernel_sz # kernel size for each stage\n",
    "        self.stride=stride # stride for each stage\n",
    "        self.padding=padding # padding for each stage\n",
    "        self.dilation=dilation # dilation for each stage\n",
    "        self.groups=groups # number of groups for each stage\n",
    "        self.droprate=droprate # constant droprate for all stage\n",
    "        self.drop_mode=drop_mode # drop_mode is same for all stage\n",
    "        self.use_se=use_se # flag for using se operation in each stage\n",
    "        self.squeeze_ratio=squeeze_ratio # squeeze_ratio is same for all stage\n",
    "        self.transition_kersz=transition_kersz # transition kernel size for each stage\n",
    "        self.transition_stride=transition_stride # transition stride for each stage\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm' or 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        self.device=device\n",
    "        self.num_stages = len(num_blocks)\n",
    "        \n",
    "        self.stem = Stem(stem_in_channels=self.input_channels, stem_out_channels=main_channels[0], stem_kernel_sz=self.stem_kersz, stem_stride=self.stem_stride,\n",
    "                         stem_padding=adjust_padding_for_strided_output(self.stem_kersz, self.stem_stride), stem_dilation=1, stem_groups=1, device=self.device)\n",
    "        self.stages = []\n",
    "        for i in range(self.num_stages):\n",
    "            if i == 0:\n",
    "                self.stages.append(Stage(transition_flag=False, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                    kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                    drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, norm_mode=self.norm_mode, device='cuda'))\n",
    "            else:\n",
    "                self.stages.append(Stage(transition_flag=True, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_dim[i],\n",
    "                                        kernel_sz=self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i], dilation=self.dilation[i], groups=self.groups[i], droprate=self.droprate[i],\n",
    "                                        drop_mode=self.drop_mode[i], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, transition_channels=self.main_channels[i-1], transition_kersz=self.transition_kersz[i],\n",
    "                                        transition_stride=self.transition_stride[i], norm_mode=self.norm_mode, device='cuda'))\n",
    "        self.stages = torch.nn.ModuleList(self.stages)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        stage_output = []\n",
    "        x = self.stem(x)\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.stages[i](x)\n",
    "            stage_output.append(x)\n",
    "        return x, stage_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output shape:  torch.Size([1, 768, 7, 21])\n",
      "encoder output: is gradient alive?:  True\n",
      "\n",
      "linearly decaying survival probability:  [[0.0, 0.027777777777777776, 0.05555555555555555], [0.08333333333333333, 0.1111111111111111, 0.1388888888888889], [0.16666666666666666, 0.19444444444444445, 0.2222222222222222, 0.25, 0.2777777777777778, 0.3055555555555556, 0.3333333333333333, 0.3611111111111111, 0.3888888888888889], [0.4166666666666667, 0.4444444444444444, 0.4722222222222222]]\n",
      "encoder output shape:  torch.Size([1, 768, 7, 21])\n",
      "shape of 0th stage output:  torch.Size([1, 96, 56, 168])\n",
      "shape of 1th stage output:  torch.Size([1, 192, 28, 84])\n",
      "shape of 2th stage output:  torch.Size([1, 384, 14, 42])\n",
      "shape of 3th stage output:  torch.Size([1, 768, 7, 21])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Encoder                                            [1, 768, 7, 21]           --\n",
       "├─Stem: 1-1                                        [1, 96, 56, 168]          --\n",
       "│    └─Conv2d: 2-1                                 [1, 96, 56, 168]          4,704\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─Stage: 2-2                                  [1, 96, 56, 168]          --\n",
       "│    │    └─ModuleList: 3-1                        --                        5,687,730\n",
       "│    └─Stage: 2-3                                  [1, 192, 28, 84]          --\n",
       "│    │    └─ModuleList: 3-2                        --                        3,746,916\n",
       "│    └─Stage: 2-4                                  [1, 384, 14, 42]          --\n",
       "│    │    └─ModuleList: 3-3                        --                        16,362,072\n",
       "│    └─Stage: 2-5                                  [1, 768, 7, 21]           --\n",
       "│    │    └─ModuleList: 3-4                        --                        16,936,848\n",
       "====================================================================================================\n",
       "Total params: 42,738,270\n",
       "Trainable params: 42,738,270\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 14.45\n",
       "====================================================================================================\n",
       "Input size (MB): 1.81\n",
       "Forward/backward pass size (MB): 422.29\n",
       "Params size (MB): 170.95\n",
       "Estimated Total Size (MB): 595.05\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_list, dp_mode = create_linear_p([3,3,9,3], 'batch', 0.5) # creates linearly decaying stochastic depth drop probability\n",
    "# dp_list, dp_mode = create_uniform_p([3,3,9,3], 'batch', 0.001) # create constant stochastic depth drop probability\n",
    "\n",
    "unet_encoder = Encoder(num_blocks=[3, 3, 9, 3], input_channels=3, stem_kersz=(4,4), stem_stride=(4,4), img_hw=[(56, 168), (28, 84), (14, 42), (7, 21)], main_channels=[96, 192, 384, 768], expansion_dim=[96*4, 192*4, 384*4, 768*4],\n",
    "                               kernel_sz=[(7,7), (7,7), (7,7), (7,7)], stride=[(1,1),(1,1),(1,1),(1,1)], padding=['same', 'same', 'same', 'same'], dilation=[1,1,1,1], groups=[1,1,1,1], droprate=dp_list, drop_mode=dp_mode,\n",
    "                               use_se=[True, True, True, True], squeeze_ratio=16, transition_kersz=[-1, (1,1),(1,1),(1,1)], transition_stride=[-1, (2,2), (2,2), (2,2)], norm_mode='layer_norm', device='cuda')\n",
    "encoder_output, stage_output = unet_encoder(ex_img)\n",
    "\n",
    "print(\"encoder_output shape: \", encoder_output.shape)\n",
    "print(\"encoder output: is gradient alive?: \", encoder_output.requires_grad)\n",
    "print()\n",
    "\n",
    "print(\"linearly decaying survival probability: \", dp_list)\n",
    "print(\"encoder output shape: \", encoder_output.shape)\n",
    "for s_index in range(len(stage_output)):\n",
    "    print(f\"shape of {s_index}th stage output: \", stage_output[s_index].shape)\n",
    "print()\n",
    "\n",
    "torchinfo.summary(unet_encoder, (1, 3, 224, 672))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before upsampling shape:  torch.Size([1, 768, 7, 21])\n",
      "after 1st upsampling shape:  torch.Size([1, 384, 14, 42])\n",
      "after 2nd upsampling shape:  torch.Size([1, 192, 28, 84])\n",
      "after 3rd upsampling shape:  torch.Size([1, 96, 56, 168])\n",
      "after 4th upsampling shape:  torch.Size([1, 48, 112, 336])\n",
      "after 5th upsampling shape:  torch.Size([1, 24, 224, 672])\n",
      "\n",
      "original image shape:  torch.Size([1, 3, 224, 672])\n",
      "\n",
      "upsampling as the way corresponding to stem:  torch.Size([1, 48, 224, 672])\n"
     ]
    }
   ],
   "source": [
    "print(\"before upsampling shape: \", encoder_output.shape)\n",
    "\n",
    "up_sample = torch.nn.ConvTranspose2d(768, 384, kernel_size=(7,7), stride=2, padding=3, output_padding=(1,1), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "up_sample_output = up_sample(encoder_output)\n",
    "print(\"after 1st upsampling shape: \", up_sample_output.shape)\n",
    "\n",
    "up_sample = torch.nn.ConvTranspose2d(384, 192, kernel_size=(7,7), stride=2, padding=3, output_padding=(1,1), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "up_sample_output = up_sample(up_sample_output)\n",
    "print(\"after 2nd upsampling shape: \", up_sample_output.shape)\n",
    "\n",
    "up_sample = torch.nn.ConvTranspose2d(192, 96, kernel_size=(7,7), stride=2, padding=3, output_padding=(1,1), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "decoder_up_sample = up_sample(up_sample_output)\n",
    "print(\"after 3rd upsampling shape: \", decoder_up_sample.shape)\n",
    "\n",
    "up_sample = torch.nn.ConvTranspose2d(96, 48, kernel_size=(7,7), stride=2, padding=3, output_padding=(1,1), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "head_up_sample = up_sample(decoder_up_sample)\n",
    "print(\"after 4th upsampling shape: \", head_up_sample.shape)\n",
    "\n",
    "up_sample = torch.nn.ConvTranspose2d(48, 24, kernel_size=(7,7), stride=2, padding=3, output_padding=(1,1), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "head_output = up_sample(head_up_sample)\n",
    "print(\"after 5th upsampling shape: \", head_output.shape)\n",
    "\n",
    "print()\n",
    "print(\"original image shape: \", ex_img.shape)\n",
    "print()\n",
    "\n",
    "# 4x increase in image width and image height\n",
    "up_sample = torch.nn.ConvTranspose2d(96, 48, kernel_size=(4,4), stride=(4,4), padding=(0,0), output_padding=(0,0), groups=1, bias=True, dilation=1, padding_mode='zeros', device='cuda')\n",
    "up_sample_output = up_sample(decoder_up_sample)\n",
    "print(\"upsampling as the way corresponding to stem: \", up_sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should carefully consider the transition layer, because when up-convolution (transposed convolution) is used, output size is doubled.\n",
    "class DBlock(torch.nn.Module):\n",
    "    def __init__(self, img_hw, in_channels, out_channels, kernel_sz, stride, padding, groups, dilation, droprate, drop_mode,\n",
    "                 use_se, squeeze_ratio, is_fuse=False, fused_channels=-1, transition=False, transition_channels=-1, transition_kersz=-1,\n",
    "                 transition_stride=-1, transition_padding=-1, transition_out_padding=-1, norm_mode='layer_norm', device='cuda'):\n",
    "        super().__init__()\n",
    "        self.img_h = img_hw[0]\n",
    "        self.img_w = img_hw[1]\n",
    "        \n",
    "        # residual path parameters\n",
    "        self.block_channels = in_channels\n",
    "        self.block_out_channels = out_channels\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.droprate = droprate\n",
    "        if drop_mode not in ['row', 'batch']:\n",
    "            raise Exception(\"drop_mode must be either 'row' or 'batch'\")\n",
    "        self.drop_mode = drop_mode\n",
    "        \n",
    "        self.use_se = use_se\n",
    "        self.squeeze_r = squeeze_ratio\n",
    "        \n",
    "        if transition and is_fuse:\n",
    "            raise Exception(\"Upsampling and encoder stage output concat cannot happen same time!\")\n",
    "        \n",
    "        self.is_fuse = is_fuse # whether to decide encoder stage output is concatenated to decoder path\n",
    "        self.fused_channels = fused_channels # number of channels from encoder output\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"UnSupported normalization method: {norm_mode}. Use either 'layer_norm' or 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        self.device = device\n",
    "        \n",
    "        # deconvolution path parameters\n",
    "        self.transition = transition\n",
    "        self.transition_channels = transition_channels\n",
    "        self.transition_kersz = transition_kersz\n",
    "        self.transition_stride = transition_stride\n",
    "        self.transition_padding = transition_padding\n",
    "        self.transition_out_padding = transition_out_padding\n",
    "        \n",
    "        if self.transition:\n",
    "            # unlike encoder, kernel_size for self.conv_1 is not same as self.kernel_sz because of multiple possible choice of padding sizes\n",
    "            self.conv_1 = torch.nn.ConvTranspose2d(in_channels=self.transition_channels, out_channels=self.block_channels, kernel_size=self.transition_kersz, \n",
    "                                                stride=self.transition_stride, padding=self.transition_padding, output_padding=self.transition_out_padding, groups=1, bias=True, dilation=1, padding_mode='zeros', device=self.device, dtype=None)\n",
    "            self.transition_conv = torch.nn.ConvTranspose2d(in_channels=self.transition_channels, out_channels=self.block_channels, kernel_size=self.transition_kersz, \n",
    "                                                stride=self.transition_stride, padding=self.transition_padding, output_padding=self.transition_out_padding, groups=1, bias=True, dilation=1, padding_mode='zeros', device=self.device, dtype=None)\n",
    "        else:\n",
    "            if self.is_fuse:\n",
    "                self.conv_1 = DepthWiseSepConv(in_channels=(self.block_channels+self.fused_channels), out_channels=self.block_channels, kernel_sz=self.kernel_sz,\n",
    "                                                        stride=self.stride, padding=self.padding, dilation=self.dilation, device=self.device)\n",
    "                # transition_conv is needed to adjust the number of output filters (pointwise convolution is used to fit the size)\n",
    "                self.transition_conv = torch.nn.Conv2d(in_channels=self.block_channels+self.fused_channels, out_channels=self.block_channels, kernel_size=(1,1),\n",
    "                                                       stride=(1,1), padding=(0,0), dilation=1, groups=1, bias=True, padding_mode='zeros', device=self.device)\n",
    "            else:\n",
    "                self.conv_1 = DepthWiseSepConv(in_channels=self.block_channels, out_channels=self.block_channels, kernel_sz=self.kernel_sz,\n",
    "                                                        stride=self.stride, padding=self.padding, dilation=self.dilation, device=self.device)\n",
    "            \n",
    "        self.pointwise_1 = torch.nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_out_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=self.groups, bias=True, padding_mode='zeros', device=self.device)\n",
    "        self.pointwise_2 = torch.nn.Conv2d(in_channels=self.block_out_channels, out_channels=self.block_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=self.groups, bias=True, padding_mode='zeros', device=self.device)\n",
    "        \n",
    "        if self.use_se:\n",
    "            self.fc1 = torch.nn.Conv2d(in_channels=self.block_channels, out_channels=int(self.block_channels/self.squeeze_r), kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=1, bias=True, device=self.device)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Conv2d(in_channels=int(self.block_channels/self.squeeze_r), out_channels=self.block_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0), dilation=1, groups=1, bias=True, device=self.device)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        # unlike batch normalization, layer normalization consumes more gpu memory as input spatial size increases.\n",
    "        # while decoder head restore image size to original, it necessarily process large spatial size input tensor. it consumes much more gpu memory\n",
    "        # than using batch normalization as normalization method. (4M vs 256M) In this case, use batch normalization instead of layer normalization.\n",
    "        if self.norm_mode == 'layer_norm':\n",
    "            self.normalization = torch.nn.LayerNorm([self.block_channels, self.img_h, self.img_w], device=self.device)\n",
    "        elif self.norm_mode == 'batch_norm':\n",
    "            self.normalization = torch.nn.BatchNorm2d(num_features=self.block_channels, device=self.device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "    \n",
    "    # encoder stage output concatenation is not happend in the block-level, do that on the stage-level\n",
    "    def forward(self, x):\n",
    "        highway = x\n",
    "    \n",
    "        x = self.conv_1(x)\n",
    "        x = self.normalization(x)\n",
    "        x = self.pointwise_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.pointwise_2(x)\n",
    "        \n",
    "        if self.use_se:\n",
    "            squeeze = torch.nn.functional.avg_pool2d(x, x.size()[2:])\n",
    "            excitation = self.fc1(squeeze)\n",
    "            excitation = self.relu(excitation)\n",
    "            excitation = self.fc2(excitation)\n",
    "            attention = self.sigmoid(excitation)\n",
    "            x = x * attention\n",
    "        \n",
    "        if self.transition or self.is_fuse:\n",
    "            highway = self.transition_conv(highway)\n",
    "        \n",
    "        highway = highway + torchvision.ops.stochastic_depth(x, p=self.droprate, mode=self.drop_mode, training=self.training)\n",
    "        return highway\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder 1st block output:  torch.Size([1, 384, 14, 42])\n",
      "decoder 1st block output: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "decoder_block = DBlock(img_hw=(14, 42), in_channels=384, out_channels=384*4, kernel_sz=(7,7), stride=(1,1), padding=(3,3), groups=1, dilation=1, \n",
    "                       droprate=0.5, drop_mode='batch', use_se=True, squeeze_ratio=16, is_fuse=False, fused_channels=-1, transition=True, transition_channels=768, transition_kersz=(7,7),\n",
    "                       transition_stride=(2,2), transition_padding=(3,3), transition_out_padding=(1,1), device='cuda')\n",
    "decoder_block_output = decoder_block(encoder_output)\n",
    "\n",
    "print(\"decoder 1st block output: \", decoder_block_output.shape)\n",
    "print(\"decoder 1st block output: is gradient alive?: \", decoder_block_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DBlock                                   [1, 384, 14, 42]          --\n",
       "├─ConvTranspose2d: 1-1                   [1, 384, 14, 42]          14,451,072\n",
       "├─LayerNorm: 1-2                         [1, 384, 14, 42]          451,584\n",
       "├─Conv2d: 1-3                            [1, 1536, 14, 42]         591,360\n",
       "├─GELU: 1-4                              [1, 1536, 14, 42]         --\n",
       "├─Conv2d: 1-5                            [1, 384, 14, 42]          590,208\n",
       "├─Conv2d: 1-6                            [1, 24, 1, 1]             9,240\n",
       "├─ReLU: 1-7                              [1, 24, 1, 1]             --\n",
       "├─Conv2d: 1-8                            [1, 384, 1, 1]            9,600\n",
       "├─Sigmoid: 1-9                           [1, 384, 1, 1]            --\n",
       "├─ConvTranspose2d: 1-10                  [1, 384, 14, 42]          14,451,072\n",
       "==========================================================================================\n",
       "Total params: 30,554,136\n",
       "Trainable params: 30,554,136\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 17.69\n",
       "==========================================================================================\n",
       "Input size (MB): 0.45\n",
       "Forward/backward pass size (MB): 14.45\n",
       "Params size (MB): 122.22\n",
       "Estimated Total Size (MB): 137.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(decoder_block, (1, 768, 7, 21)) #6,888,894 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # should carefully consider the transition layer, because when up-convolution (transposed convolution) is used, output size is doubled.\n",
    "# class DBlock(torch.nn.Module):\n",
    "#     def __init__(self, img_hw, in_channels, out_channels, kernel_sz, stride, padding, groups, dilation, droprate, drop_mode,\n",
    "#                  use_se, squeeze_ratio, is_fuse=False, fused_channels=-1, transition=False, transition_channels=-1, transition_kersz=-1,\n",
    "#                  transition_stride=-1, transition_padding=-1, transition_out_padding=-1, device='cuda'):\n",
    "\n",
    "class DStage(torch.nn.Module):\n",
    "    def __init__(self, transition_flag, num_blocks:int, img_hw:list, in_channels, out_channels, kernel_sz, stride, padding, dilation, groups,\n",
    "                 droprate, drop_mode:list, use_se:bool, squeeze_ratio:int, encoder_channels:int, transition_channels=-1, transition_kersz=-1, transition_stride=-1, \n",
    "                 transition_padding=-1, transition_out_padding=1, norm_mode='layer_norm', device='cuda'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transition_flag = transition_flag\n",
    "        self.num_blocks = num_blocks\n",
    "        self.img_h = img_hw[0]\n",
    "        self.img_w = img_hw[1]\n",
    "        self.stage_channels = in_channels\n",
    "        self.stage_out_channels = out_channels\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.droprate=droprate\n",
    "        for i in range(len(drop_mode)):\n",
    "            if drop_mode[i] not in ['row', 'batch']:\n",
    "                raise Exception(\"drop_mode must be 'row' or 'batch'\")\n",
    "        self.drop_mode = drop_mode\n",
    "        self.use_se = use_se\n",
    "        self.squeeze_r = squeeze_ratio\n",
    "        self.encoder_channels = encoder_channels\n",
    "        self.device = device\n",
    "        \n",
    "        self.transition_channels = transition_channels\n",
    "        self.transition_kersz = transition_kersz\n",
    "        self.transition_stride = transition_stride\n",
    "        self.transition_padding = transition_padding\n",
    "        self.transition_out_padding = transition_out_padding\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm' 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0 and self.transition_flag: # upsampling and encoder stage output concatenation cannot happen same time!\n",
    "                self.blocks.append(DBlock([self.img_h, self.img_w], in_channels=self.stage_channels, out_channels=self.stage_out_channels,\n",
    "                                          kernel_sz=self.kernel_sz, stride=self.stride, padding=self.padding, groups=self.groups, dilation=self.dilation,\n",
    "                                          droprate=self.droprate[i], drop_mode=self.drop_mode[i], use_se=self.use_se, squeeze_ratio=self.squeeze_r,\n",
    "                                          transition=True, transition_channels=self.transition_channels, transition_kersz=self.transition_kersz, \n",
    "                                          transition_stride=self.transition_stride, transition_padding=self.transition_padding, \n",
    "                                          transition_out_padding=self.transition_out_padding, norm_mode=self.norm_mode, device=self.device))\n",
    "            else:\n",
    "                if i==1: # after upsampling is dones, concatenation is applied to the decoder output\n",
    "                    is_fused = True # self.encoder_channels are only valid when is_fused==True\n",
    "                else:\n",
    "                    is_fused = False\n",
    "                self.blocks.append(DBlock([self.img_h, self.img_w], in_channels=self.stage_channels, out_channels=self.stage_out_channels,\n",
    "                                          kernel_sz=self.kernel_sz, stride=self.stride, padding=self.padding, groups=self.groups, dilation=self.dilation,\n",
    "                                          droprate=self.droprate[i], drop_mode=self.drop_mode[i], use_se=self.use_se, squeeze_ratio=self.squeeze_r,\n",
    "                                          is_fuse=is_fused, fused_channels=self.encoder_channels, norm_mode=self.norm_mode, device=self.device))\n",
    "        self.blocks = torch.nn.ModuleList(self.blocks)\n",
    "                \n",
    "    def forward(self, x, enc_stage_o):\n",
    "        for i in range(self.num_blocks):\n",
    "            if i == 1: # after upconvolution are applied, concatenation with encoder stage output happens\n",
    "                x = torch.concat([x, enc_stage_o], dim=1) # concatenate between decoder first block output and corresponding encoder output\n",
    "            x = self.blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder stage output shape:  torch.Size([1, 768, 7, 21])\n",
      "matched encoder stage output shape:  torch.Size([1, 384, 14, 42])\n",
      "\n",
      "decoder stage_1 output shape:  torch.Size([1, 384, 14, 42])\n",
      "matched encoder stage output shape:  torch.Size([1, 192, 28, 84])\n",
      "decoder stage_1: is gradient alive?:  True\n",
      "\n",
      "decoder stage_2 output shape:  torch.Size([1, 192, 28, 84])\n",
      "decoder stage_2: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder stage output shape: \", encoder_output.shape)\n",
    "print(\"matched encoder stage output shape: \", stage_output[-2].shape)\n",
    "print()\n",
    "decoder_stage1 = DStage(transition_flag=True, num_blocks=3, img_hw=[14, 42], in_channels=384, out_channels=384*4, kernel_sz=(7,7), stride=1,\n",
    "                       padding='same', dilation=1, groups=1, droprate=[0.5, 0.5, 0.5], drop_mode=['batch', 'batch', 'batch'], use_se=True,\n",
    "                       squeeze_ratio=16, encoder_channels=384, transition_channels=768, transition_kersz=(7,7), transition_stride=(2,2), transition_padding=(3,3),\n",
    "                       transition_out_padding=(1,1), norm_mode='layer_norm', device='cuda')\n",
    "\n",
    "decoder_stage_output = decoder_stage1(encoder_output, stage_output[-2])\n",
    "print(\"decoder stage_1 output shape: \", decoder_stage_output.shape)\n",
    "print(\"matched encoder stage output shape: \", stage_output[-3].shape)\n",
    "print(\"decoder stage_1: is gradient alive?: \", decoder_stage_output.requires_grad)\n",
    "print()\n",
    "\n",
    "decoder_stage2 = DStage(transition_flag=True, num_blocks=3, img_hw=[28, 84], in_channels=192, out_channels=192*4, kernel_sz=(7,7), stride=1,\n",
    "                       padding='same', dilation=1, groups=1, droprate=[0.5, 0.5, 0.5], drop_mode=['batch', 'batch', 'batch'], use_se=True,\n",
    "                       squeeze_ratio=16, encoder_channels=192, transition_channels=384, transition_kersz=(7,7), transition_stride=(2,2), transition_padding=(3,3),\n",
    "                       transition_out_padding=(1,1), norm_mode='layer_norm', device='cuda')\n",
    "decoder_stage_output = decoder_stage2(decoder_stage_output, stage_output[-3])\n",
    "\n",
    "print(\"decoder stage_2 output shape: \", decoder_stage_output.shape)\n",
    "print(\"decoder stage_2: is gradient alive?: \", decoder_stage_output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DStage                                   [1, 384, 14, 42]          --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─DBlock: 2-1                       [1, 384, 14, 42]          --\n",
       "│    │    └─ConvTranspose2d: 3-1         [1, 384, 14, 42]          14,451,072\n",
       "│    │    └─LayerNorm: 3-2               [1, 384, 14, 42]          451,584\n",
       "│    │    └─Conv2d: 3-3                  [1, 1536, 14, 42]         591,360\n",
       "│    │    └─GELU: 3-4                    [1, 1536, 14, 42]         --\n",
       "│    │    └─Conv2d: 3-5                  [1, 384, 14, 42]          590,208\n",
       "│    │    └─Conv2d: 3-6                  [1, 24, 1, 1]             9,240\n",
       "│    │    └─ReLU: 3-7                    [1, 24, 1, 1]             --\n",
       "│    │    └─Conv2d: 3-8                  [1, 384, 1, 1]            9,600\n",
       "│    │    └─Sigmoid: 3-9                 [1, 384, 1, 1]            --\n",
       "│    │    └─ConvTranspose2d: 3-10        [1, 384, 14, 42]          14,451,072\n",
       "│    └─DBlock: 2-2                       [1, 384, 14, 42]          --\n",
       "│    │    └─DepthWiseSepConv: 3-11       [1, 384, 14, 42]          333,696\n",
       "│    │    └─LayerNorm: 3-12              [1, 384, 14, 42]          451,584\n",
       "│    │    └─Conv2d: 3-13                 [1, 1536, 14, 42]         591,360\n",
       "│    │    └─GELU: 3-14                   [1, 1536, 14, 42]         --\n",
       "│    │    └─Conv2d: 3-15                 [1, 384, 14, 42]          590,208\n",
       "│    │    └─Conv2d: 3-16                 [1, 24, 1, 1]             9,240\n",
       "│    │    └─ReLU: 3-17                   [1, 24, 1, 1]             --\n",
       "│    │    └─Conv2d: 3-18                 [1, 384, 1, 1]            9,600\n",
       "│    │    └─Sigmoid: 3-19                [1, 384, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-20                 [1, 384, 14, 42]          295,296\n",
       "│    └─DBlock: 2-3                       [1, 384, 14, 42]          --\n",
       "│    │    └─DepthWiseSepConv: 3-21       [1, 384, 14, 42]          167,040\n",
       "│    │    └─LayerNorm: 3-22              [1, 384, 14, 42]          451,584\n",
       "│    │    └─Conv2d: 3-23                 [1, 1536, 14, 42]         591,360\n",
       "│    │    └─GELU: 3-24                   [1, 1536, 14, 42]         --\n",
       "│    │    └─Conv2d: 3-25                 [1, 384, 14, 42]          590,208\n",
       "│    │    └─Conv2d: 3-26                 [1, 24, 1, 1]             9,240\n",
       "│    │    └─ReLU: 3-27                   [1, 24, 1, 1]             --\n",
       "│    │    └─Conv2d: 3-28                 [1, 384, 1, 1]            9,600\n",
       "│    │    └─Sigmoid: 3-29                [1, 384, 1, 1]            --\n",
       "==========================================================================================\n",
       "Total params: 34,654,152\n",
       "Trainable params: 34,654,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 19.55\n",
       "==========================================================================================\n",
       "Input size (MB): 1.35\n",
       "Forward/backward pass size (MB): 46.97\n",
       "Params size (MB): 138.62\n",
       "Estimated Total Size (MB): 186.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(decoder_stage1, [(1, 768, 7, 21),(1, 384, 14, 42)]) # 34,654,152 params\n",
    "# torchinfo.summary(decoder_stage2, [(1, 384, 14, 42), (1, 192, 28, 84)]) # 11,050,980 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one transpose convolution layer (increase spatial dimension n times)\n",
    "class DStemNx(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1,\n",
    "                 dilation=1, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.out_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.device = device\n",
    "        \n",
    "        self.stem_conv = torch.nn.ConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size,\n",
    "                                                  stride=self.stride, padding=self.padding, output_padding=self.out_padding, groups=self.groups,\n",
    "                                                  bias=True, dilation=self.dilation, padding_mode='zeros', device=self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        return x\n",
    "\n",
    "# apply stacked transpose convolution layer (increase spatial dimension n * num_stacked times)\n",
    "class DStemStacked(torch.nn.Module):\n",
    "    def __init__(self, in_channels:list, out_channels:list, kernel_size:list, stride:list, padding:list, output_padding:list, groups:list,\n",
    "                 dilation:list, device):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.out_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.device = device\n",
    "        \n",
    "        self.num_stacked = len(self.in_channels)\n",
    "        \n",
    "        self.stacks = []        \n",
    "        for i in range(self.num_stacked):\n",
    "            self.stacks.append(torch.nn.ConvTranspose2d(in_channels=self.in_channels[i], out_channels=self.out_channels[i], kernel_size=self.kernel_size[i],\n",
    "                                                    stride=self.stride[i], padding=self.padding[i], output_padding=self.out_padding[i], groups=self.groups[i],\n",
    "                                                    bias=True, dilation=self.dilation[i], padding_mode='zeros', device=self.device))\n",
    "        self.stacks = torch.nn.ModuleList(self.stacks)\n",
    "        \n",
    "        self.out_conv = torch.nn.Conv2d(in_channels=self.out_channels[-1], out_channels=self.out_channels[-1], kernel_size=(1,1), stride=1,\n",
    "                                        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.stacks)):\n",
    "            x = self.stacks[i](x)\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "# class DBlock(torch.nn.Module):\n",
    "#     def __init__(self, img_hw, in_channels, out_channels, kernel_sz, stride, padding, groups, dilation, droprate, drop_mode,\n",
    "#                  use_se, squeeze_ratio, is_fuse=False, fused_channels=-1, transition=False, transition_channels=-1, transition_kersz=-1,\n",
    "#                  transition_stride=-1, transition_padding=-1, transition_out_padding=-1, device='cuda'):\n",
    "\n",
    "# continues convnext style decoder block\n",
    "class DStemStaged(torch.nn.Module):\n",
    "    def __init__(self, num_blocks:list, img_hw:list, input_channels, main_channels, expansion_channels, kernel_sz, stride, padding, dilation, groups, droprate, drop_mode,\n",
    "                 use_se, squeeze_ratio, transition_kersz, transition_stride, transition_padding, transition_out_padding, norm_mode, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.img_hw = img_hw\n",
    "        \n",
    "        self.input_channels = input_channels # number of last stage channels\n",
    "        self.main_channels = main_channels\n",
    "        self.expansion_channels = expansion_channels\n",
    "        \n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.droprate = droprate\n",
    "        self.drop_mode = drop_mode\n",
    "        \n",
    "        self.use_se = use_se\n",
    "        self.squeeze_ratio = squeeze_ratio\n",
    "        self.transition_kersz = transition_kersz\n",
    "        self.transition_stride = transition_stride\n",
    "        self.transition_padding = transition_padding\n",
    "        self.transition_out_padding = transition_out_padding\n",
    "        \n",
    "        if norm_mode not in ['batch_norm', 'layer_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm', 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.stages = []\n",
    "        self.num_stages = len(self.num_blocks)\n",
    "        for i in range(self.num_stages):\n",
    "            stage = []\n",
    "            for j in range(self.num_blocks[i]):\n",
    "                if j == 0: # for the first block\n",
    "                    if i==0:\n",
    "                        transition_channels = self.input_channels\n",
    "                    elif i!=0:\n",
    "                        transition_channels = self.main_channels[i-1]\n",
    "                    stage.append(DBlock(img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_channels[i], kernel_sz=self.kernel_sz[i], \n",
    "                                        stride=self.stride[i], padding=self.padding[i], groups=self.groups[i], dilation=self.dilation[i], droprate=self.droprate[i][j],\n",
    "                                        drop_mode=self.drop_mode[i][j], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, transition=True,\n",
    "                                        transition_channels=transition_channels, transition_kersz=self.transition_kersz[i], transition_stride=self.transition_stride[i],\n",
    "                                        transition_padding=self.transition_padding[i], transition_out_padding=self.transition_out_padding[i], norm_mode=self.norm_mode, device=self.device))\n",
    "                else: # transition (up-sampling) does not happen if it is not first block\n",
    "                    stage.append(DBlock(img_hw=self.img_hw[i], in_channels=self.main_channels[i], out_channels=self.expansion_channels[i], kernel_sz=self.kernel_sz[i], \n",
    "                                        stride=self.stride[i], padding=self.padding[i], groups=self.groups[i], dilation=self.dilation[i], norm_mode=self.norm_mode, droprate=self.droprate[i][j],\n",
    "                                        drop_mode=self.drop_mode[i][j], use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio))\n",
    "        \n",
    "            self.stages.append(torch.nn.ModuleList(stage))\n",
    "        self.stages = torch.nn.ModuleList(self.stages)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.stages)):\n",
    "            for j in range(len(self.stages[i])):\n",
    "                x = self.stages[i][j](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DBlock(torch.nn.Module):\n",
    "#     def __init__(self, img_hw, in_channels, out_channels, kernel_sz, stride, padding, groups, dilation, droprate, drop_mode,\n",
    "#                  use_se, squeeze_ratio, is_fuse=False, fused_channels=-1, transition=False, transition_channels=-1, transition_kersz=-1,\n",
    "#                  transition_stride=-1, transition_padding=-1, transition_out_padding=-1, device='cuda'):\n",
    "\n",
    "# class DStage(torch.nn.Module):\n",
    "#     def __init__(self, transition_flag, num_blocks:int, img_hw:list, in_channels, out_channels, kernel_sz, stride, padding, dilation, groups,\n",
    "#                  droprate, drop_mode:list, use_se:bool, squeeze_ratio:int, encoder_channels:int, transition_channels=-1, transition_kersz=-1, transition_stride=-1, \n",
    "#                  transition_padding=-1, transition_out_padding=1, device='cuda'):\n",
    "\n",
    "# decoder head (this does not need to be symmetrical with encoder)\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_blocks:list, img_hw:list, main_channels:list, expansion_dim:list, kernel_sz:list, \n",
    "                 stride:list, padding:list, dilation:list, groups:list, droprate:list, drop_mode:list, \n",
    "                 use_se:list, squeeze_ratio:int, encoder_channels:list,  transition_kersz:list, transition_stride:list, \n",
    "                 transition_padding:list, transition_out_padding:list, norm_mode:str, head:torch.nn.ModuleList, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks # list that contains number of blocks for each stage\n",
    "        self.img_hw = img_hw # list that contains representative (img_height, img_width) for each stage\n",
    "        self.main_channels = main_channels\n",
    "        self.expansion_dim = expansion_dim\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.droprate = droprate # stochastic depth drop probability ex) [[], [], []] -> outer: number of stages, inner: number of blocks\n",
    "        self.drop_mode = drop_mode # stochastic depth drop mode\n",
    "        \n",
    "        self.use_se = use_se  # list that contains whether to use sequeeze and excitation module in the stage\n",
    "        self.squeeze_ratio = squeeze_ratio\n",
    "        \n",
    "        self.encoder_channels = encoder_channels # list that contains number of channels for each encoder stage output (this excludes final encoder stage output)\n",
    "        \n",
    "        self.transition_kersz = transition_kersz\n",
    "        self.transition_stride = transition_stride\n",
    "        self.transition_padding = transition_padding\n",
    "        self.transition_out_padding = transition_out_padding\n",
    "        \n",
    "        if norm_mode not in ['layer_norm', 'batch_norm']:\n",
    "            raise Exception(f\"Unsupported normalization method: {norm_mode}. Must be either 'layer_norm', 'batch_norm'\")\n",
    "        self.norm_mode = norm_mode\n",
    "        \n",
    "        # head is intentionally created outside of Decoder class because many variations and img size can be required in various situation.\n",
    "        self.head = head # decoder head for further convolution\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.num_stages = len(self.num_blocks) # length of list indicates the number of stages\n",
    "        self.stages = []        \n",
    "        \n",
    "        for i in range(self.num_stages):\n",
    "            if i == 0: # start of decoder first inputs the number of last encoder stage output channels\n",
    "                transition_channels = self.encoder_channels[0]\n",
    "            else:\n",
    "                transition_channels = self.main_channels[i-1]\n",
    "            \n",
    "            # unlike encoder, decoder always upsampling the input (transition_flag should be always True)\n",
    "            self.stages.append(DStage(transition_flag=True, num_blocks=self.num_blocks[i], img_hw=self.img_hw[i], in_channels=self.main_channels[i],\n",
    "                                        out_channels=self.expansion_dim[i], kernel_sz = self.kernel_sz[i], stride=self.stride[i], padding=self.padding[i],\n",
    "                                        dilation = self.dilation[i], groups=self.groups[i], droprate=self.droprate[i], drop_mode=self.drop_mode[i],\n",
    "                                        use_se=self.use_se[i], squeeze_ratio=self.squeeze_ratio, encoder_channels=self.encoder_channels[i+1], transition_channels=transition_channels, \n",
    "                                        transition_kersz=self.transition_kersz[i], transition_stride=self.transition_stride[i], transition_padding=self.transition_padding[i],\n",
    "                                        transition_out_padding=self.transition_out_padding[i], norm_mode=self.norm_mode, device=self.device))\n",
    "        self.stages = torch.nn.ModuleList(self.stages)\n",
    "        \n",
    "    def forward(self, x, enc_stage_out):\n",
    "        encoder_idx = len(enc_stage_out)-2 # -1 from zero indexing, -1 from skipping last stage output\n",
    "        for i in range(self.num_stages):\n",
    "            if encoder_idx >= 0:\n",
    "                x = self.stages[i](x, enc_stage_out[encoder_idx])\n",
    "            encoder_idx -= 1\n",
    "\n",
    "        x = self.head(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DStemNx                                  [1, 128, 224, 672]        --\n",
       "├─ConvTranspose2d: 1-1                   [1, 128, 224, 672]        196,736\n",
       "==========================================================================================\n",
       "Total params: 196,736\n",
       "Trainable params: 196,736\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 29.61\n",
       "==========================================================================================\n",
       "Input size (MB): 3.61\n",
       "Forward/backward pass size (MB): 154.14\n",
       "Params size (MB): 0.79\n",
       "Estimated Total Size (MB): 158.54\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class DStem4x(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1,\n",
    "#                  dilation=1, device='cuda'):\n",
    "\n",
    "# class DStemStacked(torch.nn.Module):\n",
    "#     def __init__(self, in_channels:list, out_channels:list, kernel_size:list, stride:list, padding:list, output_padding:list, groups:list,\n",
    "#                  dilation:list, device):\n",
    "\n",
    "# class DStemStaged(torch.nn.ModuleList):\n",
    "#     def __init__(self, num_blocks:list, img_hw:list, input_channels, main_channels, expansion_channels, kernel_sz, stride, padding, dilation, groups, droprate, drop_mode,\n",
    "#                  use_se, squeeze_ratio, transition_kersz, transition_stride, transition_padding, transition_out_padding, device='cuda'):\n",
    "\n",
    "head_4x = DStemNx(in_channels=96, out_channels=128, kernel_size=(4,4), stride=(4,4), padding=(0,0), output_padding=(0,0), groups=1, dilation=1, device='cuda')\n",
    "# head_stack = DStemStacked(in_channels=[96, 128], out_channels=[128, 256], kernel_size=[(4,4), (12,12)], stride=[(2,4), (4,4)], padding=[(1,0), (1,0)], output_padding=[(0,0), (0,0)], \n",
    "#                           groups=[1,1], dilation=[1,1], device='cuda')\n",
    "\n",
    "# droprate, drop_mode = create_linear_p([3,9,3,2,2], dp_mode='batch', last_p=0.5)\n",
    "# head_stage = DStemStaged(num_blocks=[2,2], img_hw=[(112, 336), (224, 672)], input_channels=96, main_channels=[96, 128], expansion_channels=[96*4, 128*4], \n",
    "#                          kernel_sz=[(7,7), (7,7)], stride=[(1,1), (1,1)], padding=['same', 'same'], dilation=[1, 1], groups=[1,1], \n",
    "#                          droprate=droprate[3:], drop_mode=drop_mode[3:],\n",
    "#                          use_se=[True, True], squeeze_ratio=16, transition_kersz=[(7,7), (7,7)], transition_stride=[(2,2), (2,2)], transition_padding=[(3,3), (3,3)],\n",
    "#                          transition_out_padding=[(1,1), (1,1)], norm_mode='batch_norm', device='cuda')\n",
    "\n",
    "\n",
    "torchinfo.summary(head_4x, (1, 96, 56, 168)) # 196,736 params\n",
    "# torchinfo.summary(head_stack, (1, 96, 73, 102)) # 4,981,376 params\n",
    "# torchinfo.summary(head_stage, (1, 96, 73, 102)) # 2,564,476 params, much heavier than head_4x but entire number of params are not that huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder output shape:  torch.Size([1, 128, 224, 672])\n",
      "decoder output: is gradient alive?:  True\n"
     ]
    }
   ],
   "source": [
    "# without considering stem layers\n",
    "# unet_decoder = Decoder(num_blocks=[3,9,3], img_hw=[(14, 42), (28, 84), (56, 168)], main_channels=[384, 192, 96], expansion_dim=[384*4, 192*4, 96*4],\n",
    "#                   kernel_sz=[(7,7)]*3, stride=[(1,1)]*3, padding=['same']*3, dilation=[1]*3, groups=[1]*3, droprate=droprate[:3], drop_mode=drop_mode[:3],\n",
    "#                   use_se=[True]*3, squeeze_ratio=16, encoder_channels=[768, 384, 192, 96], transition_kersz=[(7,7)]*3, transition_stride=[(2,2)]*3, \n",
    "#                   transition_padding=[(3,3)]*3, transition_out_padding=[(1,1)]*3, norm_mode='layer_norm', head=head_stage, device='cuda')\n",
    "\n",
    "\n",
    "droprate, drop_mode = create_linear_p([3,9,3], dp_mode='batch', last_p=0.5)\n",
    "unet_decoder = Decoder(num_blocks=[3,9,3], img_hw=[(14, 42), (28, 84), (56, 168)], main_channels=[384, 192, 96], expansion_dim=[384*4, 192*4, 96*4],\n",
    "                  kernel_sz=[(7,7)]*3, stride=[(1,1)]*3, padding=['same']*3, dilation=[1]*3, groups=[1]*3, droprate=droprate, drop_mode=drop_mode,\n",
    "                  use_se=[True]*3, squeeze_ratio=16, encoder_channels=[768, 384, 192, 96], transition_kersz=[(7,7)]*3, transition_stride=[(2,2)]*3, \n",
    "                  transition_padding=[(3,3)]*3, transition_out_padding=[(1,1)]*3, norm_mode='layer_norm', head=head_4x, device='cuda')\n",
    "\n",
    "\n",
    "decoder_output = unet_decoder(encoder_output, stage_output)\n",
    "print(\"decoder output shape: \", decoder_output.shape) # restore to original image size\n",
    "print(\"decoder output: is gradient alive?: \", decoder_output.requires_grad)\n",
    "# torchinfo.summary(decoder, [(1, 768, 10, 13), (1, 96, 73, 102), (1, 192, 37, 51), (1, 384, 19, 26), (1, 768, 10, 13)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container encoder and decoder\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, num_cls:int, output_mode:str):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.num_cls = num_cls # number of segmentation class\n",
    "        \n",
    "        if output_mode not in ['logits', 'probs']:\n",
    "            raise Exception(f\"Unsupported output mode: {output_mode}. Must be either 'logits' or 'probs'\")\n",
    "        self.output_mode = output_mode\n",
    "        \n",
    "        if self.output_mode == 'probs':\n",
    "            self.softmax = torch.nn.Softmax2d()\n",
    "        \n",
    "        if isinstance(self.decoder.head, DStemNx):\n",
    "            in_channels = self.decoder.head.out_channels\n",
    "        elif isinstance(self.decoder.head, DStemStacked):\n",
    "            in_channels = self.decoder.head.out_channels[-1]\n",
    "        elif isinstance(self.decoder.head, DStemStaged):\n",
    "            in_channels = self.decoder.head.main_channels[-1]\n",
    "        else:\n",
    "            raise Exception(\"Not Implemented: Currently Head supports only DStemNx, DStemStacked, DStemStaged\")\n",
    "        \n",
    "        self.cls_head = torch.nn.Conv2d(in_channels=in_channels, out_channels=self.num_cls, kernel_size=(1,1),\n",
    "                                        stride=(1,1), padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=encoder.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, stage_o = self.encoder(x)\n",
    "        x = self.decoder(x, stage_o)\n",
    "        x = self.cls_head(x)  # (B, num_cls, ori_h, ori_w)\n",
    "        \n",
    "        if self.output_mode=='probs':\n",
    "            x = self.softmax(x) # return probabilities\n",
    "        \n",
    "        del stage_o\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet output shape:  torch.Size([1, 5, 224, 672])\n",
      "unet output: is gradient alive?:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "UNet                                                    [1, 5, 224, 672]          --\n",
       "├─Encoder: 1-1                                          [1, 768, 7, 21]           --\n",
       "│    └─Stem: 2-1                                        [1, 96, 56, 168]          --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 96, 56, 168]          4,704\n",
       "│    └─ModuleList: 2-2                                  --                        --\n",
       "│    │    └─Stage: 3-2                                  [1, 96, 56, 168]          5,687,730\n",
       "│    │    └─Stage: 3-3                                  [1, 192, 28, 84]          3,746,916\n",
       "│    │    └─Stage: 3-4                                  [1, 384, 14, 42]          16,362,072\n",
       "│    │    └─Stage: 3-5                                  [1, 768, 7, 21]           16,936,848\n",
       "├─Decoder: 1-2                                          [1, 128, 224, 672]        --\n",
       "│    └─ModuleList: 2-3                                  --                        --\n",
       "│    │    └─DStage: 3-6                                 [1, 384, 14, 42]          34,654,152\n",
       "│    │    └─DStage: 3-7                                 [1, 192, 28, 84]          18,554,028\n",
       "│    │    └─DStage: 3-8                                 [1, 96, 56, 168]          7,512,690\n",
       "│    └─DStemNx: 2-4                                     [1, 128, 224, 672]        --\n",
       "│    │    └─ConvTranspose2d: 3-9                        [1, 128, 224, 672]        196,736\n",
       "├─Conv2d: 1-3                                           [1, 5, 224, 672]          645\n",
       "├─Softmax2d: 1-4                                        [1, 5, 224, 672]          --\n",
       "=========================================================================================================\n",
       "Total params: 103,656,521\n",
       "Trainable params: 103,656,521\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 107.80\n",
       "=========================================================================================================\n",
       "Input size (MB): 1.81\n",
       "Forward/backward pass size (MB): 1084.64\n",
       "Params size (MB): 414.63\n",
       "Estimated Total Size (MB): 1501.07\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnext_unet = UNet(encoder=unet_encoder, decoder=unet_decoder, num_cls=5, output_mode='probs')\n",
    "unet_o = convnext_unet(ex_img)\n",
    "print(\"unet output shape: \", unet_o.shape)\n",
    "print(\"unet output: is gradient alive?: \", unet_o.requires_grad)\n",
    "\n",
    "torchinfo.summary(convnext_unet, (1, 3, 224, 672)) # 106,024,261 params total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
